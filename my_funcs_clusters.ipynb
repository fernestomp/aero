{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook de funciones que usan los demás notebooks. Este debe contener las fuciones actualizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "def raw_to_datafr (xlsPath, xlsPathMfgCurve):\n",
    "    \"\"\"\n",
    "\n",
    "    :param xlsPath: path to data file\n",
    "    :param xlsPathMfgCurve: path to manufacturer power curve file\n",
    "    :return: processed dataframes\n",
    "    \"\"\"\n",
    "    #imprimir a consola\n",
    "    os.write(1, b\"Inciando procesamiento de datos...\\n\")\n",
    "\n",
    "    \n",
    "    #xlsPath = 'C:/Users/mungu/Documents/DatosWTG.xlsx'\n",
    "    #xlsPath = 'C:\\\\Users\\\\ernesto\\\\Dropbox\\\\Doctorado\\\\datos\\\\DatosWTG.xlsx'\n",
    "    #xlsPathMfgCurve = 'Curva de potencia vestas 90.xlsx'\n",
    "\n",
    "    #dataVPxls = pd.read_excel(xlsPath,usecols=[0,1,2],index_col=0,names=['vViento','Pacw'])\n",
    "    dataVPxls = pd.read_excel(xlsPath,usecols=[0,1,2],index_col=0)\n",
    "    dataVPxls.columns =['vViento','Pacw']\n",
    "    #agrego la columna de potencia instantanea sin filtrar\n",
    "    #dataVPxls['Pw']= (dataVPxls.iloc[1:,1].values-dataVPxls.iloc[0:-1,1]) * np.pi*45**2\n",
    "    print('Total de registros: ' + str(len(dataVPxls)))\n",
    "    #dfMfgCurve = pd.read_excel(xlsPathMfgCurve,usecols=[0,2],index_col=0,names=['pw'])#cambio esto en la nueva version\n",
    "    dfMfgCurve = pd.read_excel(xlsPathMfgCurve,usecols=[0,2],index_col=0)\n",
    "    dfMfgCurve.columns = ['pw']\n",
    "    #marcando los datos faltantes asignando un nan a la fila completa\n",
    "    datamk = dataVPxls\n",
    "    datamk.loc[datamk.isnull().any(axis=1), :] = np.nan\n",
    "    #numero de filas sin datos\n",
    "    print('Numero de filas sin datos')\n",
    "    print(datamk.loc[datamk.isnull().any(axis=1), :].isnull().sum())\n",
    "\n",
    "    #eliminando filas con NaN. Si busco la fecha anterior debe aparecer error.\n",
    "    cleanData = datamk.dropna()\n",
    "\n",
    "    ###calculando la potencia\n",
    "    #la pontencia del archivo de excel es la densidad de potencia acumulada.\n",
    "    #Se tiene que hacer la resta de la potencia siguiente a la anterio y multiplicar por pi*r^2\n",
    "    #el radio es 45m\n",
    "    #si se hace la resta con un array de numpy (values) si se puede restar \n",
    "    #pues se hace elemento por elemento\n",
    "    dataVP = cleanData.drop('Pacw',axis=1)\n",
    "    dataVP['Pw']= (cleanData.iloc[1:,1].values-cleanData.iloc[0:-1,1]) * np.pi*45**2\n",
    "\n",
    "    #eliminar el ultimo valor pues es NaN\n",
    "    dataVP.drop(dataVP.tail(1).index,inplace=True) \n",
    "\n",
    "    #Eliminando outliers\n",
    "    dataVP.drop([pd.to_datetime('2016-03-07 09:50:00')],inplace=True)\n",
    "    #agregado para el reporte\n",
    "    dataVP.drop([pd.to_datetime('2016-03-08 09:00:00')],inplace=True)\n",
    "    dataVP.drop([pd.to_datetime('2016-01-02 06:40:00')],inplace=True)\n",
    "    dataVP.drop([pd.to_datetime('2016-05-05 07:20:00')],inplace=True)\n",
    "    dataVP.drop([pd.to_datetime('2016-05-23 23:00:00')],inplace=True)\n",
    "\n",
    "\n",
    "    #leyendo la direccion del viento\n",
    "    #dataDirVxls = pd.read_excel(xlsPath,sheet_name=1, usecols=[0,1],index_col=0,names=['Dir'])\n",
    "    dataDirVxls = pd.read_excel(xlsPath,sheet_name=1, usecols=[0,1],index_col=0)\n",
    "    dataDirVxls.columns =['Dir']\n",
    "    #limpiando datos\n",
    "    #marcando los datos faltantes asignando un nan a la fila completa\n",
    "    datamkdir = dataDirVxls\n",
    "    datamkdir.loc[datamkdir.isnull().any(axis=1), :] = np.nan\n",
    "    #eliminando filas con NaN. Si busco la fecha anterior debe aparecer error.\n",
    "    #tambien elimino el ultimo valor como lo hice en los datos de v y p\n",
    "    #el copy es para que no me de la copy warning\n",
    "    dataDir = datamkdir.dropna().copy()\n",
    "    dataDir.drop(dataDir.tail(1).index,inplace=True) \n",
    "    #eliminando del outlier de viento misma fecha que el de la pontencia\n",
    "    dataDir.drop([pd.to_datetime('2016-03-07 09:50:00')],inplace=True)\n",
    "    #agregado para el reporte\n",
    "    dataDir.drop([pd.to_datetime('2016-03-08 09:00:00')],inplace=True)\n",
    "    dataDir.drop([pd.to_datetime('2016-01-02 06:40:00')],inplace=True)\n",
    "    dataDir.drop([pd.to_datetime('2016-05-05 07:20:00')],inplace=True)\n",
    "    dataDir.drop([pd.to_datetime('2016-05-23 23:00:00')],inplace=True)\n",
    "\n",
    "    direcvrad= np.deg2rad(dataDir['Dir'].values)\n",
    "    velocidades = dataVP.iloc[:]['vViento'].values\n",
    "    vecVel = [-np.sin(direcvrad)*velocidades,np.cos(direcvrad)*velocidades]\n",
    "    vecVelnp=np.array(vecVel).transpose()\n",
    "    #original sin timestamp\n",
    "    #dfVecVel = pd.DataFrame(data=vecVelnp,columns=['vx','vy']\n",
    "    #con timestamp\n",
    "    dfVecVel = pd.DataFrame(data=vecVelnp,columns=['vx','vy'],index=dataVP.index)\n",
    "\n",
    "    #datos direccion velocidad\n",
    "    #print(len(dataVP))\n",
    "    #print(len(dataDir))\n",
    "    #dataDV = pd.concat([dataDir,dataVP.vViento],axis=1)\n",
    "    dataVDP = pd.concat([dataVP.vViento,dataDir,dataVP.Pw],axis=1)\n",
    "    #dataVcD =pd.concat([dataDir,dfVecVel],axis=1)\n",
    "    #Datos en p.u.. Divido todos los datos entre el máximo del conjunto de datos\n",
    "\n",
    "\n",
    "    #OPERATIONAL DATA\t\n",
    "    # Rated power\n",
    "    # 2,000 kW/2,200 kW\n",
    "    # Cut-in wind speed\t4 m/s\n",
    "    # Cut-out wind speed\t25 m/s\n",
    "    # Re cut-in wind speed\t23 m/s\n",
    "\n",
    "\n",
    "    os.write(1, b\"Fin del procesamiento de datos\\n\")\n",
    "    return [dataVDP,dfVecVel,dfMfgCurve]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear plot interactivo para cluster y subcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"'''\n",
    "TODO:\n",
    "    -que de una opción de mostrar solo viento o potencia aunque tenga subclusters\n",
    "    -que funcione si solo incluyo dfclvv aunque tenga subclusters\n",
    "    -que imprima a consola con OS\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import RadioButtons, TextBox\n",
    "from IPython.display import display as wgdisplay\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import datetime\n",
    "from matplotlib.ticker import EngFormatter\n",
    "import matplotlib.patheffects as path_effects #efectos de texto\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import os\n",
    "sns.set()\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "class PlotSubClusterInt:\n",
    "    \"\"\"\n",
    "    Descripcion:\n",
    "    Clase para dibujar dos subplots, uno vx vs vy y otro vviento vs Pw.\n",
    "\n",
    "    Metodos:\n",
    "        createPlot: Construye el plot de matplotlib.\n",
    "            self.dfclvv: dataframe multiindice con dos columnas vx y vy agrupadas por cluster\n",
    "            dfclpw: dataframe multiindice con dos columnas vviento y Pw agrupadas por cluster\n",
    "            cl_method: string que almacena el metodo usado para hacer el cluster\n",
    "                     |   C1    |   C2   |...\n",
    "                    ----------------------\n",
    "            Timestamp|  vx vy  |  vx vy |...            datavp: dataframe original de los datos de\n",
    "\n",
    "        updatePlot: actuliza el plot en \"tiempo real\" segun los valores de los controles\n",
    "\n",
    "        onClick: Es el la funcion que esta ligada al click en el plot. Solo ejecuta annotatePlot.\n",
    "\n",
    "        annotatePlot: Crea una anotacion en el plot que indica donde el nombre del cluster mas cercano al conjunto de\n",
    "              coordenadas donde se hizo click en el plot VP. Ademas muestra el cluster en el plot VV.\n",
    "\n",
    "        blinkCluster: Resalta el cluster al que se hace referencia.\n",
    "\n",
    "    Argumentos:\n",
    "        cl_scl_order: el orden en que se hicieron los clusters (viento,viento), (viento,potencia),\n",
    "            (potencia,viento),(potencia,potencia)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.save_fig = None\n",
    "        self.text_log = None\n",
    "        self.pw_col_name = None\n",
    "        self.wind_col_name = None\n",
    "        self.axvp = None\n",
    "        self.formatterPw = None\n",
    "        self.fig = None\n",
    "        self.axvv = None\n",
    "        self.tbFilePath = None\n",
    "        self.wchkcls = None\n",
    "        self.tbreta = None\n",
    "        self.wradText = None\n",
    "        self.chkShowCPotFab = None\n",
    "        self.chkShowBetz = None\n",
    "        self.chkLimGlob = None\n",
    "        self.chkShowCnt = None\n",
    "        self.wdgPSize = None\n",
    "        self.btnSelNoneChk = None\n",
    "        self.btnSelAllChk = None\n",
    "        self.btnUpdate = None\n",
    "        self.vxmaxGlob = None\n",
    "        self.pmaxGlob = None\n",
    "        self.vvmaxGlob = None\n",
    "        self.vvminGlob = None\n",
    "        self.pminGlob = None\n",
    "        self.vymaxGlob = None\n",
    "        self.vyminGlob = None\n",
    "        self.vxminGlob = None\n",
    "        self.cl_avail = None\n",
    "        self.clnames_all = None\n",
    "        self.n_tot_clusters = None\n",
    "        self.idx_centroids_sc = None\n",
    "        self.idx_centroids = None\n",
    "        self.dfclvp = None\n",
    "        self.n_clusters = None\n",
    "        self.showLegends = None\n",
    "        self.savepath = None\n",
    "        self.showlBetz = None\n",
    "        self.showCent = None\n",
    "        self.showMfgCurve = None\n",
    "        self.showOpts = None\n",
    "        self.vvento = None\n",
    "        self.PMaxViento = None\n",
    "        self.cl_scl_order = None\n",
    "        from colorsys import hls_to_rgb\n",
    "        self.fignum=999\n",
    "        self.filename=None\n",
    "        self.fisize = (5, 5)\n",
    "        self.fontsize = 13\n",
    "        self.labelFontSize = 13\n",
    "        self.tickFontSize = 12\n",
    "        self.markerSize = 100\n",
    "        self.fontNameLabel = {'fontname':'Times New Roman'}\n",
    "        self.fontNameCluster = {'fontname':'Arial'}\n",
    "        self.ticks_font = font_manager.FontProperties(family='Times New Roman', style='normal',\n",
    "                                                      size=self.labelFontSize, weight='normal', stretch='normal')\n",
    "        self.n_subclu = 0\n",
    "        colors1 = plt.cm.tab20(np.linspace(0., 1, 20))\n",
    "        colors2 = plt.cm.Spectral(np.linspace(0, 1, 10))\n",
    "        colorstab = np.vstack((colors1, colors2))\n",
    "        self.mapa_colores=colorstab\n",
    "        self.fisize = None\n",
    "        self.dfclvv = None\n",
    "        self.datavp = None\n",
    "        self.dfMfgCurve=None\n",
    "        self.dfclvp_is_empty = True\n",
    "        #este backend junto con %matplotlib notebook\n",
    "        #hacen que el plot se actualice bien y aparecen\n",
    "        #los controles\n",
    "        #matplotlib.use('nbAgg')\n",
    "\n",
    "    def create_plot(self, dfclvv, dfclvp=None,datavp=None,figsize=(5, 5), cl_scl_order=(None, None),\n",
    "                    idx_centroids=None,idx_centroids_sc=None,fign=999, save_folder='', showCent =True,\n",
    "                    showlBetz = False,showMfgCurve=False, showOpt = 'Magnitud',dfMfgCurve=None,\n",
    "                    showLegends = True, wind_col_name ='vwind',pw_col_name='pw',save_fig=False, filename = 'None'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param filename: Nombre de archivo con el que se guarda la figura.\n",
    "        :param pw_col_name: Nombre que tiene el encabezado de la columna de potencia.\n",
    "        :param wind_col_name: Nombre que tiene el encabezado de la columna de viento.\n",
    "        :param dfclvv: df que contiene los clusters de viento en forma de Cx(vx,vy)\n",
    "        :param dfclvp: df que contiene los clusters de potencia en forma de Cx(vv,pw)\n",
    "        :param datavp: df con los datos de viento y potencia.\n",
    "        :param figsize: tamaño de la figura\n",
    "        :param cl_scl_order: En el caso de ser doble clusterizado, es el orden en que se hizo el clusterizado.\n",
    "        :param idx_centroids: contiene los identificadores de los centroides, tales como, el numero de cluster, la\n",
    "        posicion (vx,vy) del centroide, etc.\n",
    "        :param idx_centroids_sc: contiene los identificadores de los centroides de los subclusters, tales como, el\n",
    "        numero de cluster, la posicion (vx,vy) del centroide, etc.\n",
    "        :param fign: numero de figura.\n",
    "        :param save_folder: ruta donde se guardará la imagen del plot, sin el nombre de la figura.\n",
    "        :param showCent: Valor boleano que define si se mostrarán los centroides o no en el plot.\n",
    "        :param showlBetz: Valor boleano que define si se mostrará el límite de Beltz o no.\n",
    "        :param showMfgCurve: Valor boleano que define si se mostrará la curva del fabricante o no.\n",
    "        :param showOpt: Pertenece al widget option, que da la opción de mostrar el número de cluster, la magnitud o\n",
    "        el nombre del cluster.\n",
    "        :param dfMfgCurve: df que contiene los datos de viento y potencia de la curva del fabricante.\n",
    "        :param showLegends: Mostrar las leyendas o no.\n",
    "        \"\"\"\n",
    "        self.dfMfgCurve=dfMfgCurve\n",
    "        if self.dfMfgCurve is None:\n",
    "            print('Manufacturer power curve missing')\n",
    "        self.showLegends = showLegends\n",
    "        self.cl_scl_order = cl_scl_order\n",
    "        self.save_fig=save_fig\n",
    "        self.dfclvp =dfclvp\n",
    "        self.dfclvv = dfclvv\n",
    "        self.datavp = datavp\n",
    "        self.n_clusters = len(self.dfclvv.columns.levels[0])\n",
    "        self.n_tot_clusters =self.n_clusters\n",
    "        self.fignum = fign\n",
    "        self.wind_col_name = wind_col_name\n",
    "        self.pw_col_name = pw_col_name\n",
    "        self.idx_centroids = idx_centroids\n",
    "        self.idx_centroids_sc = idx_centroids_sc\n",
    "        self.save_folder = save_folder\n",
    "        self.filename = filename\n",
    "        self.showlBetz = showlBetz # show Betz's limit\n",
    "        self.showCent = showCent # show clusters centroids\n",
    "        self.showMfgCurve = showMfgCurve #show manufacturer's curve\n",
    "        #which value is shown in the plot magnitude,cluster number,cluster name, none\n",
    "        self.showOpts = showOpt\n",
    "\n",
    "        #check if power data was passed\n",
    "        if self.dfclvp is not None:\n",
    "            self.dfclvp_is_empty = False\n",
    "\n",
    "        ##################################\n",
    "        #        limite de betz          #\n",
    "        ##################################\n",
    "        if not self.dfclvp_is_empty:\n",
    "            A=np.pi*45**2\n",
    "            Cp = 0.59 #limite de Betz\n",
    "            rho = 1.1349\n",
    "            self.vvento = np.unique(datavp[wind_col_name].values)\n",
    "            self.PMaxViento = 1/2*rho*A*self.vvento**3*Cp\n",
    "\n",
    "        #########################################\n",
    "        #           CREAR PLOT                  #\n",
    "        #########################################\n",
    "\n",
    "        #para poner el plot dentro de un widget\n",
    "        #self.output = widgets.Output()\n",
    "        plt.ioff()\n",
    "        self.fig = plt.figure(self.fignum, figsize=self.fisize,constrained_layout=True)\n",
    "        #1k, 1M\n",
    "        self.formatterPw = EngFormatter(places=1, sep=\"\\N{THIN SPACE}\")  # U+2009\n",
    "        # 3 because is cluster,subcluster,vx and vy\n",
    "        if len(dfclvv.columns[0]) == 3:\n",
    "            #last column has the last subcluster name SCn, this gives n\n",
    "            self.n_subclu=int(dfclvv.columns[-1][1][2:])\n",
    "\n",
    "        if not self.dfclvp_is_empty:\n",
    "            self.axvv = self.fig.add_subplot(121)\n",
    "            if self.n_subclu > 0:  # existen subclusters , sino seria 2\n",
    "                #numero total de clusters incluidos los subclusters\n",
    "                self.n_tot_clusters = self.n_subclu*self.n_clusters\n",
    "                # solo aplica cuando hay subclusters\n",
    "                lev0 = self.dfclvv.columns.get_level_values(0)\n",
    "                lev1 = self.dfclvp.columns.get_level_values(1)\n",
    "                namcl = [(lev0[i],lev1[i]) for i in range(len(lev0))]\n",
    "                #self.clnames_all = sorted(namcl[::2]) #todos los clusters y subclusters disponibles\n",
    "                self.clnames_all = namcl[::2] #todos los clusters y subclusters disponibles\n",
    "                #ordenar\n",
    "                vv = [self.idx_centroids_sc.loc[c][wind_col_name] for c in self.clnames_all]\n",
    "                idx = np.argsort(vv)\n",
    "                x=[self.clnames_all[i] for i in idx]\n",
    "                self.clnames_all=x.copy()\n",
    "                del x\n",
    "                #nombre de los clusters disponibles sin el nombre de los subclusters\n",
    "                #self.cl_avail =sorted(set(item[0] for item in self.clnames_all))\n",
    "                self.cl_avail =set(item[0] for item in self.clnames_all)\n",
    "                #### BUSCAR MINIMOS Y MAXIMOS GLOBALES  (hacerlo más elegante)\n",
    "                l=[self.dfclvv[cl].min() for cl in self.clnames_all]\n",
    "                self.vxminGlob,self.vyminGlob =   np.amin(l,axis=0)\n",
    "                l=[self.dfclvv[cl].max() for cl in self.clnames_all]\n",
    "                self.vxmaxGlob,self.vymaxGlob =   np.amax(l,axis=0)\n",
    "\n",
    "            else:  # no hay subcluster\n",
    "                #lista ordenada con numeros y letras\n",
    "                #self.clnames_all=sorted(dfclvv.columns.levels[0],\n",
    "                #                        key=lambda x: int(\"\".join([i for i in x if i.isdigit()])))\n",
    "                self.clnames_all = self.dfclvv_in.columns.levels[0]\n",
    "                #self.cl_avail =sorted(set(item for item in self.clnames_all))\n",
    "                self.cl_avail =set(item for item in self.clnames_all)\n",
    "            l=[self.dfclvp[cl].min() for cl in self.clnames_all]\n",
    "            self.vvminGlob,self.pminGlob =   np.amin(l,axis=0)\n",
    "            l=[self.dfclvp[cl].max() for cl in self.clnames_all]\n",
    "            self.vvmaxGlob,self.pmaxGlob =   np.amax(l,axis=0)\n",
    "        else:\n",
    "            self.axvv = self.fig.add_subplot()\n",
    "            #lista ordenada con numeros y letras\n",
    "            self.clnames_all=sorted(dfclvv.columns.levels[0],\n",
    "                                    key=lambda x: int(\"\".join([i for i in x if i.isdigit()])))\n",
    "            #self.cl_avail =sorted(set(item for item in self.clnames_all))\n",
    "            self.cl_avail =set(item for item in self.clnames_all)\n",
    "        #### BUSCAR MINIMOS Y MAXIMOS GLOBALES  (hacerlo más elegante)\n",
    "        l=[self.dfclvv[cl].min() for cl in self.clnames_all]\n",
    "        self.vxminGlob,self.vyminGlob =   np.amin(l,axis=0)\n",
    "        l=[self.dfclvv[cl].max() for cl in self.clnames_all]\n",
    "        self.vxmaxGlob,self.vymaxGlob =   np.amax(l,axis=0)\n",
    "\n",
    "        if not self.dfclvp_is_empty:\n",
    "            self.axvp = self.fig.add_subplot(122)\n",
    "            self.axvp.yaxis.set_major_formatter(self.formatterPw)\n",
    "\n",
    "\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             WIDGETS                                        #\n",
    "        ##############################################################################\n",
    "\n",
    "        self.btnUpdate = widgets.Button(description='Actualizar')\n",
    "\n",
    "        self.btnUpdate.on_click(self.update_plot)\n",
    "        self.btnSelAllChk = widgets.Button(description='Sel. todo')\n",
    "        self.btnSelAllChk.on_click(self.sel_all_chk)\n",
    "        self.btnSelNoneChk = widgets.Button(description='Des. todo')\n",
    "        self.btnSelNoneChk.on_click(self.sel_none_chk)\n",
    "        self.wdgPSize = widgets.IntSlider(\n",
    "            value=2,\n",
    "            min=1,\n",
    "            max=20,\n",
    "            step=1,\n",
    "            description='Point size:',\n",
    "            continuous_update=False)\n",
    "        self.chkLimGlob = widgets.Checkbox(\n",
    "            value=True, description='Límites Globales')\n",
    "        self.chkShowCnt = widgets.Checkbox(\n",
    "            value=self.showCent, description='Mostrar centroides')\n",
    "        self.chkShowBetz = widgets.Checkbox(\n",
    "            value=self.showlBetz, description='Mostrar línea Betz')\n",
    "        self.chkShowCPotFab = widgets.Checkbox(\n",
    "            value=self.showMfgCurve, description='Mostrar curva Fab.')\n",
    "        if not self.dfclvp_is_empty:\n",
    "            if self.dfMfgCurve is not None:\n",
    "                self.chkShowCPotFab.disabled= False\n",
    "            else:\n",
    "                self.chkShowCPotFab.disabled= True\n",
    "        else:\n",
    "            self.chkShowBetz.disabled=True\n",
    "\n",
    "\n",
    "        self.tbreta = widgets.IntText(\n",
    "            value=0, description='Retardo:', layout=Layout(width='90%', height='80px'))\n",
    "        self.wradText = widgets.RadioButtons(\n",
    "            options=['Magnitud', 'Numero', 'Clusters', 'Ninguno'],\n",
    "            description='Mostrar texto:')\n",
    "        self.wradText.value= self.showOpts\n",
    "        # figsave_folder = self.save_folder\n",
    "        # figsavetime = datetime.datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S_%f\")\n",
    "        # if self.filename is None:\n",
    "        #     #poner como nombre del archivo los datos de la creacion de la imagen\n",
    "        #     figsavename = 'clustersplot'+str(self.n_clusters)+'SCl'+str(self.n_subclu)+'_'\n",
    "        #\n",
    "        # else:\n",
    "        #     figsavename = self.filename\n",
    "        self.tbFilePath = widgets.Text(description='Save path:', layout=Layout(width='90%', height='80px'))\n",
    "        self.chkShowCnt.observe(self.update_plot, 'value')\n",
    "        self.wdgPSize.observe(self.update_plot, 'value')\n",
    "        self.wradText.observe(self.update_plot, 'value')\n",
    "        self.chkShowBetz.observe(self.update_plot, 'value')\n",
    "        self.chkShowCPotFab.observe(self.update_plot, 'value')\n",
    "\n",
    "\n",
    "        self.wchkcls=[] #lista de checbox con los nombresde los clusters\n",
    "        n=1#para numerar la lista de clusters\n",
    "        for i in range(len(self.clnames_all)):\n",
    "            self.wchkcls.append(widgets.Checkbox(\n",
    "                value=True, description=str(n) +'-' +str(self.clnames_all[i])))\n",
    "            n+=1\n",
    "        box_layout = Layout(display='flex',\n",
    "                            flex_flow='column',\n",
    "                            align_items='stretch',\n",
    "                            height='200px',\n",
    "                            )\n",
    "        vbchkcls = widgets.VBox(self.wchkcls, layout=box_layout)\n",
    "\n",
    "        self.text_log=widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Log',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "        # -------------   WIDGETS EN CAJAS ----------------------\n",
    "        vbopt1 = widgets.VBox([self.chkShowCnt,self.chkShowBetz, self.chkShowCPotFab,self.chkLimGlob, self.tbreta])\n",
    "        vbopt2 = widgets.VBox([self.wradText, self.wdgPSize ])\n",
    "        vbButtons = widgets.VBox([self.btnSelAllChk,self.btnSelNoneChk, self.btnUpdate, self.tbFilePath])\n",
    "        vblog = widgets.VBox([self.text_log])\n",
    "        items = [vbButtons, vbchkcls,vbopt1,vbopt2]\n",
    "        hb = widgets.HBox(items)\n",
    "        wgdisplay(hb)\n",
    "        wgdisplay(vblog)\n",
    "        #wgdisplay(self.fig)\n",
    "        #####################################\n",
    "        #               PLOTs               #\n",
    "        #####################################\n",
    "\n",
    "        self.update_plot(1)\n",
    "\n",
    "    def sel_all_chk(self,val):\n",
    "        \"\"\"\n",
    "\n",
    "        :param self:\n",
    "        :param val:\n",
    "        \"\"\"\n",
    "        for item in self.wchkcls:\n",
    "            item.value=True\n",
    "    def sel_none_chk(self,val):\n",
    "        \"\"\"\n",
    "\n",
    "        :param self:\n",
    "        :param val:\n",
    "        \"\"\"\n",
    "        for item in self.wchkcls:\n",
    "            item.value=False\n",
    "    def save_plot(self,val):\n",
    "        \"\"\"\n",
    "        Guarda la imagen del plot.\n",
    "        :param self:\n",
    "        :param val:\n",
    "        \"\"\"\n",
    "        self.text_log.value ='Guardando plot...'\n",
    "        #check variable names, etc..\n",
    "        #si no tiene el caracter diagonal al final, lo pone\n",
    "        if self.save_folder.find('/') ==-1:\n",
    "            self.save_folder+= '/'\n",
    "        #revisar si la carpeta existe\n",
    "        if not os.path.isdir(self.save_folder):\n",
    "            #crear carpeta\n",
    "            os.mkdir(self.save_folder)\n",
    "            text = 'El directorio no existe. ' + self.save_folder + ' ha sido creado.'\n",
    "            os.write(1,bytes(text))\n",
    "\n",
    "        if self.filename is None:\n",
    "            #poner como nombre del archivo los datos de la creacion de la imagen\n",
    "            figsavename = 'clustersplot'+str(self.n_clusters)+'SCl'+str(self.n_subclu)+'_'\n",
    "        else:\n",
    "            figsavename = self.filename\n",
    "        figsavetime = datetime.datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S_%f\")\n",
    "        self.tbFilePath = widgets.Text(\n",
    "            value=self.save_folder+figsavename+ '_' +figsavetime+'.jpg',\n",
    "            description='Save path:', layout=Layout(width='90%', height='80px'))\n",
    "        plt.savefig(  self.tbFilePath.value, bbox_inches='tight', pad_inches=0.1)\n",
    "        #print('Saved in ' +  self.tbFilePath.value)\n",
    "        self.text_log.value ='Plot guardado en: ' + self.tbFilePath.value\n",
    "    def update_plot(self, val):\n",
    "        \"\"\"\n",
    "        :param self:\n",
    "        :param val:\n",
    "        \"\"\"\n",
    "        self.text_log.value = 'Actualizando plot...'\n",
    "        pSize = self.wdgPSize.value\n",
    "        self.axvv.axes.clear()\n",
    "        self.axvv.grid(visible = True)\n",
    "\n",
    "        if not self.dfclvp_is_empty:\n",
    "            self.axvp.axes.clear()\n",
    "            self.axvp.grid()\n",
    "\n",
    "        if self.n_subclu>0 :  # existen subclusters\n",
    "            clnames= [eval(el.description.split('-')[1]) for el in self.wchkcls if el.value==True]\n",
    "            #ordenar subclusters por velocidad\n",
    "            #los subclusters se ordenan dentro de cada grupo de clusters en idx_centrods_sc\n",
    "            #hay que ordenarlos tambien globalmente y no solo localmente\n",
    "            #self.cl_avail =sorted(set(item[0] for item in clnames))\n",
    "            vv = [self.idx_centroids_sc.loc[c][self.wind_col_name] for c in clnames]\n",
    "            idx = np.argsort(vv)\n",
    "            x=[clnames[i] for i in idx]\n",
    "            clnames=x.copy()\n",
    "            del x\n",
    "            #self.clnames_all =clnames# REV: SE OCUPA CLNAMES_ALL???\n",
    "            self.cl_avail =set(item[0] for item in clnames)\n",
    "\n",
    "        else :\n",
    "            clnames=[el.description.split('-')[1] for el in self.wchkcls if el.value==True]\n",
    "            #aqui tambien va por si se eliminan todos los subclusters del mismo cluster. Solo lista los clusters\n",
    "            #self.cl_avail =sorted(set(item for item in clnames))\n",
    "            self.cl_avail =set(item for item in clnames)\n",
    "\n",
    "        #################### DEFINIR LIMITES DE PLOT #######################\n",
    "\n",
    "        #buscar los minimos y maximos de los clusters\n",
    "        #debede haber una forma mas elegante de hacerlo. Como hago sliced elmultiindex con tuplas\n",
    "\n",
    "        lvxmin=np.empty(self.n_tot_clusters)\n",
    "        lvxmin.fill(np.nan)\n",
    "        lvymin=np.empty(self.n_tot_clusters)\n",
    "        lvymin.fill(np.nan)\n",
    "        lvxmax=np.empty(self.n_tot_clusters)\n",
    "        lvxmax.fill(np.nan)\n",
    "        lvymax=np.empty(self.n_tot_clusters)\n",
    "        lvymax.fill(np.nan)\n",
    "        lvvmin=np.empty(self.n_tot_clusters)\n",
    "        lvvmin.fill(np.nan)\n",
    "        lvvmax=np.empty(self.n_tot_clusters)\n",
    "        lvvmax.fill(np.nan)\n",
    "        lpmin =np.empty(self.n_tot_clusters)\n",
    "        lpmin.fill(np.nan)\n",
    "        lpmax= np.empty(self.n_tot_clusters)\n",
    "        lpmax.fill(np.nan)\n",
    "        n=0\n",
    "        for cl in clnames:\n",
    "            lvxmin[n],lvymin[n]= self.dfclvv[cl].min()\n",
    "            lvxmax[n],lvymax[n] = self.dfclvv[cl].max()\n",
    "            if not self.dfclvp_is_empty:\n",
    "                lvvmin[n],lpmin[n] = self.dfclvp[cl].min()\n",
    "                lvvmax[n],lpmax[n] = self.dfclvp[cl].max()\n",
    "            n+=1\n",
    "        vxmin=np.nanmin(lvxmin)\n",
    "        if np.isnan(vxmin):\n",
    "            vxmin=0\n",
    "        vymin = np.nanmin(lvymin)\n",
    "        if np.isnan(vymin):\n",
    "            vymin =0\n",
    "        vxmax = np.nanmax(lvxmax)\n",
    "        if np.isnan(vxmax):\n",
    "            vxmax=1\n",
    "        vymax = np.nanmax(lvymax)\n",
    "        if np.isnan(vymax):\n",
    "            vymax=1\n",
    "\n",
    "        if not self.dfclvp_is_empty:\n",
    "            vvmin = np.nanmin(lvvmin)\n",
    "            if np.isnan(vvmin):\n",
    "                vvmin=0\n",
    "            vvmax = np.nanmax(lvvmax)\n",
    "            if np.isnan(vvmax):\n",
    "                vvmax=1\n",
    "            pmin= np.nanmin(lpmin)\n",
    "            if np.isnan(pmin):\n",
    "                pmin=0\n",
    "            pmax = np.nanmax(lpmax)\n",
    "            if np.isnan(pmax):\n",
    "                pmax=1\n",
    "\n",
    "        if self.chkLimGlob.value:#plotear con limites globales o con los limites del cluster\n",
    "            self.axvv.set_xlim((self.vxminGlob, self.vxmaxGlob))\n",
    "            self.axvv.set_ylim((self.vyminGlob, self.vymaxGlob))\n",
    "            if not self.dfclvp_is_empty:\n",
    "                self.axvp.set_xlim((self.vvminGlob, self.vvmaxGlob))\n",
    "                self.axvp.set_ylim((self.pminGlob, self.pmaxGlob))\n",
    "        else:\n",
    "            self.axvv.set_xlim((vxmin, vxmax))\n",
    "            self.axvv.set_ylim((vymin, vymax))\n",
    "            if not self.dfclvp_is_empty:\n",
    "                self.axvp.set_xlim((vvmin, vvmax))\n",
    "                self.axvp.set_ylim((pmin, pmax))\n",
    "\n",
    "\n",
    "        #####################   PLOTEAR   #########################\n",
    "\n",
    "        for item in clnames:\n",
    "            #busca el cluster actual y devuelve el indice dentro de la lista de clusters donde lo encuentra\n",
    "            #es decir, asocia un numero unico a un nombre de cluster\n",
    "            #es para que el color de los clusters sea el mismo siempre\n",
    "            idxClName = [ncl_ for ncl_, clname_ in enumerate(self.clnames_all) if clname_ == item]\n",
    "            #self.fig.suptitle('Grupos de velocidad de viento y potencia', y=1)\n",
    "            # magnitud del vector\n",
    "\n",
    "            magni = round(\n",
    "                np.mean(\n",
    "                    np.sqrt(self.dfclvv[item].vx**2 +\n",
    "                            self.dfclvv[item].vy**2)),\n",
    "                1)  # magnitud de la vv\n",
    "\n",
    "            self.axvv.scatter(\n",
    "                self.dfclvv[item].vx,\n",
    "                self.dfclvv[item].vy,\n",
    "                s=pSize,\n",
    "                c=self.mapa_colores[idxClName],\n",
    "                alpha=1)\n",
    "            if not self.dfclvp_is_empty:\n",
    "                self.axvp.scatter(\n",
    "                    self.dfclvp[item][self.wind_col_name],\n",
    "                    self.dfclvp[item][self.pw_col_name],\n",
    "                    s=pSize,\n",
    "                    c=self.mapa_colores[idxClName],\n",
    "                    alpha=1)\n",
    "\n",
    "            ###########################  MOSTRAR TEXTO ###########################\n",
    "            if self.wradText.index == 0: #magnitud\n",
    "\n",
    "                text = self.axvv.text(\n",
    "                    self.dfclvv[item].vx.mean(),\n",
    "                    self.dfclvv[item].vy.mean(),\n",
    "                    magni,\n",
    "                    fontsize=self.fontsize,\n",
    "                    weight='bold',\n",
    "                    color='w',\n",
    "                    alpha=1,\n",
    "                    zorder=100,\n",
    "                    **self.fontNameCluster\n",
    "                )\n",
    "                text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='k'),\n",
    "                                       path_effects.Normal()])\n",
    "\n",
    "                if not self.dfclvp_is_empty:\n",
    "                    text = self.axvp.text(\n",
    "                        self.dfclvp[item][self.wind_col_name].mean(),\n",
    "                        self.dfclvp[item][self.pw_col_name].mean(),\n",
    "                        magni,\n",
    "                        fontsize=self.fontsize,\n",
    "                        weight='bold',\n",
    "                        color='w',\n",
    "                        alpha=1,\n",
    "                        zorder=100,\n",
    "                        **self.fontNameCluster\n",
    "                    )\n",
    "                    text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='k'),\n",
    "                                           path_effects.Normal()])\n",
    "\n",
    "            elif self.wradText.index == 1: #numero\n",
    "                text = self.axvv.text(\n",
    "                    self.dfclvv[item].vx.mean(),\n",
    "                    self.dfclvv[item].vy.mean(),\n",
    "                    idxClName[0]+1,\n",
    "                    fontsize=self.fontsize,\n",
    "                    weight='bold',\n",
    "                    color='w',\n",
    "                    alpha=1,\n",
    "                    zorder=100,\n",
    "                    **self.fontNameCluster\n",
    "                )\n",
    "                text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='k'),\n",
    "                                       path_effects.Normal()])\n",
    "\n",
    "                if not self.dfclvp_is_empty:\n",
    "                    text = self.axvp.text(\n",
    "                        self.dfclvp[item][self.wind_col_name].mean(),\n",
    "                        self.dfclvp[item][self.pw_col_name].mean(),\n",
    "                        idxClName[0]+1,\n",
    "                        fontsize=self.fontsize,\n",
    "                        weight='bold',\n",
    "                        color='w',\n",
    "                        alpha=1,\n",
    "                        zorder=100,\n",
    "                        **self.fontNameCluster\n",
    "                    )\n",
    "                    text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='k'),\n",
    "                                           path_effects.Normal()])\n",
    "\n",
    "            elif self.wradText.index == 2:\n",
    "                text = self.axvv.text(\n",
    "                    self.dfclvv[item].vx.mean(),\n",
    "                    self.dfclvv[item].vy.mean(),\n",
    "                    item,\n",
    "                    fontsize=self.fontsize,\n",
    "                    weight='bold',\n",
    "                    color='w',\n",
    "                    alpha=1,\n",
    "                    zorder=100,\n",
    "                    **self.fontNameCluster\n",
    "                )\n",
    "                text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='k'),\n",
    "                                       path_effects.Normal()])\n",
    "\n",
    "                if not self.dfclvp_is_empty:\n",
    "                    text = self.axvp.text(\n",
    "                        self.dfclvp[item][self.wind_col_name].mean(),\n",
    "                        self.dfclvp[item][self.pw_col_name].mean(),\n",
    "                        item,\n",
    "                        fontsize=self.fontsize,\n",
    "                        weight='bold',\n",
    "                        color='w',\n",
    "                        alpha=1,\n",
    "                        zorder=100,\n",
    "                        **self.fontNameCluster\n",
    "                    )\n",
    "                    text.set_path_effects([path_effects.Stroke(linewidth=2, foreground='k'),\n",
    "                                           path_effects.Normal()])\n",
    "\n",
    "        #################### MOSTRAR CENTROIDES#######################\n",
    "        if self.chkShowCnt.value:\n",
    "\n",
    "            for cl in self.cl_avail:\n",
    "\n",
    "                #el nombred el cluster esta en el indice\n",
    "                #le quito la letra con cl[1:] y dejo solo el numero como\n",
    "                #esta en el dataframe\n",
    "                numcl = int(cl[1:])\n",
    "                self.axvv.scatter(\n",
    "                    self.idx_centroids.loc[numcl].vx,\n",
    "                    self.idx_centroids.loc[numcl].vy,\n",
    "                    marker='X',\n",
    "                    edgecolor='black',\n",
    "                    linewidth=1,\n",
    "                    facecolor='yellow',\n",
    "                    s=self.markerSize)\n",
    "                if not self.dfclvp_is_empty:\n",
    "                    self.axvp.scatter(\n",
    "                        self.idx_centroids.loc[numcl][self.wind_col_name],\n",
    "                        self.idx_centroids.loc[numcl][self.pw_col_name],\n",
    "                        marker='X',\n",
    "                        edgecolor='black',\n",
    "                        linewidth=1,\n",
    "                        facecolor='yellow',\n",
    "                        s=self.markerSize)\n",
    "\n",
    "            ################# MOSTRAR CENTROIDES SUBCLUSTERS #####################\n",
    "            if self.n_subclu > 0:\n",
    "                for el in clnames:\n",
    "                    self.axvv.scatter(\n",
    "                        self.idx_centroids_sc.loc[el].vx,\n",
    "                        self.idx_centroids_sc.loc[el].vy,\n",
    "                        marker='h',\n",
    "                        edgecolor='black',\n",
    "                        linewidth=1,\n",
    "                        facecolor='aqua',\n",
    "                        s=self.markerSize)\n",
    "                    if not self.dfclvp_is_empty:\n",
    "                        self.axvp.scatter(\n",
    "                            self.idx_centroids_sc.loc[el][self.wind_col_name],\n",
    "                            self.idx_centroids_sc.loc[el][self.pw_col_name],\n",
    "                            marker='h',\n",
    "                            edgecolor='black',\n",
    "                            linewidth=1,\n",
    "                            facecolor='aqua',\n",
    "                            s=self.markerSize)\n",
    "        ################## MOSTRAR CURVA DEL FABRICANTE #####################\n",
    "        if not self.dfclvp_is_empty:\n",
    "            if self.chkShowCPotFab.value:\n",
    "                self.axvp.plot(self.dfMfgCurve.index, self.dfMfgCurve.pw, c='red', label='Manufacturer')\n",
    "        ################## MOSTRAR LIMITE DE BETZ ###########################\n",
    "        if not self.dfclvp_is_empty:\n",
    "            if self.chkShowBetz.value:\n",
    "                self.axvp.plot(self.vvento,self.PMaxViento,label='Betz',c='y')\n",
    "        ################# CONFIGURAR TEXTOS DEL PLOT #########################\n",
    "        self.axvv.set_xlabel('vx [m/s]',fontsize = self.labelFontSize, **self.fontNameLabel)\n",
    "        self.axvv.set_ylabel('vy [m/s]',fontsize = self.labelFontSize, **self.fontNameLabel)\n",
    "        self.axvv.tick_params(axis='both', which='major')\n",
    "\n",
    "        #con esto cambio el texto de las thicks a el que defino en self.tick_font\n",
    "        for label in self.axvv.get_xticklabels():\n",
    "            label.set_fontproperties(self.ticks_font)\n",
    "        for label in self.axvv.get_yticklabels():\n",
    "            label.set_fontproperties(self.ticks_font)\n",
    "\n",
    "        if not self.dfclvp_is_empty:\n",
    "            self.axvp.set_xlabel('Wind Speed [m/s]',fontsize = self.labelFontSize, **self.fontNameLabel)\n",
    "            self.axvp.set_ylabel('Power [W]',fontsize = self.labelFontSize, **self.fontNameLabel)\n",
    "            self.axvp.tick_params(axis='both', which='major')\n",
    "            for label in self.axvp.get_xticklabels():\n",
    "                label.set_fontproperties(self.ticks_font)\n",
    "            for label in self.axvp.get_yticklabels():\n",
    "                label.set_fontproperties(self.ticks_font)\n",
    "        ################### MOSTRAR LEYENDA ################################\n",
    "        if self.showLegends:\n",
    "            legCentroids = mlines.Line2D([], [], color='yellow', marker='X', linestyle='None',\n",
    "                                         markersize=10, label='Centroids',markeredgecolor='black',markeredgewidth=1.5)\n",
    "            if self.n_subclu > 0:\n",
    "                legSecCentroids = mlines.Line2D([], [], color='aqua', marker='h', linestyle='None',\n",
    "                                                markersize=10, label='Sec. centroids',markeredgecolor='black',markeredgewidth=1)\n",
    "                if not self.dfclvp_is_empty:\n",
    "                    self.axvp.legend(handles=[legCentroids, legSecCentroids],facecolor = 'gainsboro',\n",
    "                                     frameon=True, loc='upper left')\n",
    "                self.axvv.legend(handles=[legCentroids, legSecCentroids],facecolor = 'gainsboro',\n",
    "                                 frameon=True, loc='upper right')\n",
    "            else:\n",
    "                self.axvv.legend(handles=[legCentroids],facecolor = 'gainsboro',frameon=True, loc='upper right')\n",
    "                if not self.dfclvp_is_empty:\n",
    "                    self.axvp.legend(handles=[legCentroids],facecolor = 'gainsboro',frameon=True, loc='upper left')\n",
    "        ################### MOSTRAR PLOT #################################\n",
    "        self.fig.canvas.toolbar_position= 'left'\n",
    "        self.fig.canvas.toolbar_visible = True\n",
    "        # Disable the resizing feature\n",
    "        self.fig.canvas.resizable = True\n",
    "        # If true then scrolling while the mouse is over the canvas will not move the entire notebook\n",
    "        self.fig.canvas.capture_scroll = True\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "        self.text_log.value = 'Plot actualizado.'\n",
    "        if self.save_fig:\n",
    "            self.save_plot(val)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataframe to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TO DO: en el idx_centroids los numeros de los clusters no llevan la C, en el idx_centroids_sc si\n",
    "la llevan, o cambiar todo a que la lleven o que no la lleven'''\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from scipy.spatial.distance import cdist\n",
    "#crear clusters a partir de dataframes\n",
    "def dataframe_to_cluster(dfvxvy, n_clusters,dfVP =None, n_subclu=0, clusters_data=None,subclusters_data=None,\n",
    "                         datadir=None):\n",
    "    \"\"\"\n",
    "        Descripcion:\n",
    "        Esta funcion toma diferentes dataframes de entrada agrupa los datos por cluster.\n",
    "\n",
    "        :param subclusters_data: data to do subclustering [wind,pow,dir]\n",
    "        :param clusters_data: data to do clustering [wind,pow,dir]\n",
    "        :param dfvxvy: dataframe con columnas vx y vy\n",
    "        :param dfVP: dataframe con columnas de magnitud de viento y potencia\n",
    "        :param n_clusters: numero de clusters\n",
    "        :param n_subclu: Número de suclusters a calcular a partir de los n clusters calculados en un principio.\n",
    "                  Si n_subclu=0 no se calcula ningun subcluster. Por defecto n_sub=0\n",
    "\n",
    "\n",
    "        :return cl_ord: un array que contiene el número de cluster n ordenado de menor a mayor\n",
    "            de las magnitudes de la velocidad de viento sin tomar en cuenta la direccion\n",
    "        :return dfclvv: dataframe donde las componentes de velocidad de viento vx y vy estan\n",
    "            agrupadas por cluster\n",
    "        :return dfclpw: dataframe donde la potencia esta agrupada por cluster\n",
    "        :return dfclvp: dataframe donde la potencia y la magnitud de viento esta agrupada por\n",
    "            cluster\n",
    "    \"\"\"\n",
    "    #check if dataframe with power values exist\n",
    "    if dfVP is None:\n",
    "        dfvp_is_empty = True\n",
    "    else:\n",
    "        dfvp_is_empty = False\n",
    "    #'''- KMEANS -----------------------------------------------------------------------------------------'''\n",
    "    if clusters_data=='wind':\n",
    "        kmeans = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(dfvxvy)\n",
    "        #buscar centroides en dataframes\n",
    "        #https://stackoverflow.com/questions/42583995/get-the-centroid-row-index-from-k-means-clustering-using-sklearn\n",
    "        min_dist = np.min(cdist(dfvxvy.values, kmeans.cluster_centers_, 'euclidean'), axis=1)#distancia minima a cada centroide\n",
    "        Y = pd.DataFrame(min_dist, index=dfvxvy.index, columns=['PCTimeStamp'])\n",
    "        Z = pd.DataFrame(kmeans.labels_, index=dfvxvy.index, columns=['cluster_ID'])\n",
    "        #crea el dataframe\n",
    "        PAP = pd.concat([Y,Z], axis=1)\n",
    "\n",
    "    if clusters_data=='pow' and not dfvp_is_empty:\n",
    "\n",
    "        kmeans = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(dfVP)\n",
    "        #buscar centroides en dataframes\n",
    "        min_dist = np.min(cdist(dfVP.values, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "        Y = pd.DataFrame(min_dist, index=dfVP.index, columns=['PCTimeStamp'])\n",
    "        Z = pd.DataFrame(kmeans.labels_, index=dfVP.index, columns=['cluster_ID'])\n",
    "        PAP = pd.concat([Y,Z], axis=1)\n",
    "\n",
    "    elif clusters_data=='pow' and  dfvp_is_empty:\n",
    "\n",
    "        print('Power data is emtpy...')\n",
    "        return None\n",
    "\n",
    "    if clusters_data=='dir' and not dfvp_is_empty:\n",
    "        kmeans = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(datadir)\n",
    "        #buscar centroides en dataframes\n",
    "        min_dist = np.min(cdist(dfVP.values, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "        Y = pd.DataFrame(min_dist, index=dfVP.index, columns=['PCTimeStamp'])\n",
    "        Z = pd.DataFrame(kmeans.labels_, index=dfVP.index, columns=['cluster_ID'])\n",
    "        PAP = pd.concat([Y,Z], axis=1)\n",
    "\n",
    "    elif clusters_data =='dir' and dfvp_is_empty:\n",
    "\n",
    "        print('Clustering with only direction and no power data not implemented yet.')\n",
    "        return None\n",
    "\n",
    "    #ordend e los cluster por magnitud de vv\n",
    "    cl_magni = np.zeros(n_clusters)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        vx = dfvxvy.vx.values[kmeans.labels_ == i]\n",
    "        vy = dfvxvy.vy.values[kmeans.labels_ == i]\n",
    "        cl_magni[i] = np.mean(np.sqrt(vx**2 + vy**2))  #magnitud de la vv\n",
    "    cl_ord = np.argsort(cl_magni.argsort()) #ver https://github.com/numpy/numpy/issues/8757\n",
    "\n",
    "    grouped = PAP.groupby(['cluster_ID'])#agrupa por numero de clusters las distancias minimas\n",
    "    idx_centroids = grouped.idxmin()#encuentra el indice de la distanciam minima\n",
    "    #agregando columnas de velocidadd de viento y potencia al dataframe de los centroides\n",
    "    vxvy=dfvxvy.loc[idx_centroids.PCTimeStamp].values\n",
    "    if dfvp_is_empty:\n",
    "        idx_centroids=idx_centroids.assign(vx=vxvy[:,0],vy=vxvy[:,1])\n",
    "    else:\n",
    "        vvpot = dfVP.loc[idx_centroids.PCTimeStamp].values\n",
    "        idx_centroids=idx_centroids.assign(vx=vxvy[:,0],vy=vxvy[:,1],vwind=vvpot[:,0],pw=vvpot[:,1])\n",
    "        idx_centroids.sort_values(by='vwind',inplace=True)# ordenar por velocidad de viento\n",
    "\n",
    "\n",
    "    idx_centroids.reset_index(inplace=True) #que ya no sea el cluster id el indice\n",
    "    idx_centroids.index.set_names('cluster_ID_ord',inplace=True)  #ponerle el nombre al indice ordenado\n",
    "    idx_centroids.index+=1 #paraque el indice empieze en uno y no exista cluster 0\n",
    "    #agregando columna de magnitud de vv\n",
    "    idx_centroids['wind_mag'] = cl_magni\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## CREAR MULTIINDICE ---------------------------------------------------------------'''\n",
    "    #CLUSTER VX,VY:\n",
    "    #https://stackoverflow.com/questions/37835508/how-to-do-multi-column-from-tuples\n",
    "    #nombre de las columnas del dataframe\n",
    "    if dfvp_is_empty:\n",
    "        colheadvv = []\n",
    "        for i in range(n_clusters):\n",
    "            colheadvv.append(('C' + str(i + 1), 'vx'))\n",
    "            colheadvv.append(('C' + str(i + 1), 'vy'))\n",
    "        dfclvv = pd.DataFrame()\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            dfclvv = pd.concat([\n",
    "                dfclvv, dfvxvy.vx[kmeans.labels_ ==np.where(cl_ord ==i)[0][0]],\n",
    "                dfvxvy.vy[kmeans.labels_ == np.where(cl_ord ==i)[0][0]]\n",
    "            ],\n",
    "                axis=1,\n",
    "                ignore_index=True)\n",
    "        #crear multiindice\n",
    "        dfclvv.columns = pd.MultiIndex.from_tuples(colheadvv)\n",
    "\n",
    "    else:\n",
    "\n",
    "        colheadvv = []\n",
    "        colheadpw = []\n",
    "        for i in range(n_clusters):\n",
    "            colheadvv.append(('C' + str(i + 1), 'vx'))\n",
    "            colheadvv.append(('C' + str(i + 1), 'vy'))\n",
    "            colheadpw.append('C' + str(i + 1))\n",
    "        dfclvv = pd.DataFrame()\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            dfclvv = pd.concat([\n",
    "                dfclvv, dfvxvy.vx[kmeans.labels_ ==np.where(cl_ord ==i)[0][0]],\n",
    "                dfvxvy.vy[kmeans.labels_ == np.where(cl_ord ==i)[0][0]]\n",
    "            ],\n",
    "                axis=1,\n",
    "                ignore_index=True)\n",
    "        #crear multiindice\n",
    "        dfclvv.columns = pd.MultiIndex.from_tuples(colheadvv)\n",
    "\n",
    "        #CLUSTER POTENCIA:\n",
    "        dfclpw = pd.DataFrame()\n",
    "        for i in range(n_clusters):\n",
    "            dfclpw = pd.concat([dfclpw, dfVP.Pw[kmeans.labels_ == np.where(cl_ord ==i)[0][0]]],\n",
    "                               ignore_index=True,\n",
    "                               axis=1)\n",
    "        dfclpw.columns = colheadpw\n",
    "\n",
    "        #CLUSTER VIENTO POTENCIA:\n",
    "\n",
    "        colheadvp = []\n",
    "        for i in range(n_clusters):\n",
    "            colheadvp.append(('C' + str(i + 1), 'vViento'))\n",
    "            colheadvp.append(('C' + str(i + 1), 'Pw'))\n",
    "        dfclvp = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            dfclvp = pd.concat([\n",
    "                dfclvp, dfVP.vViento[kmeans.labels_ == np.where(cl_ord ==i)[0][0]],\n",
    "                dfVP.Pw[kmeans.labels_ ==np.where(cl_ord ==i)[0][0]]] ,\n",
    "                axis=1,\n",
    "                ignore_index=True)\n",
    "        #crear multiindice\n",
    "        dfclvp.columns = pd.MultiIndex.from_tuples(colheadvp)\n",
    "\n",
    "    if clusters_data=='direccion':\n",
    "        #CLUSTER direccion viento\n",
    "        colheadvp = []\n",
    "        for i in range(n_clusters):\n",
    "            colheadvp.append(('C' + str(i + 1), 'Dir'))\n",
    "            colheadvp.append(('C' + str(i + 1), 'vViento'))\n",
    "        dfcldv = pd.DataFrame()\n",
    "        for i in range(n_clusters):\n",
    "            dfcldv = pd.concat([\n",
    "                dfcldv, datadir.vViento[kmeans.labels_ ==( cl_ord ==i)],\n",
    "                datadir.Dir[kmeans.labels_ == (cl_ord ==i)]],\n",
    "                axis=1,ignore_index=True)\n",
    "        #crear multiindice\n",
    "        dfcldv.columns = pd.MultiIndex.from_tuples(colheadvp)\n",
    "    '''- CALCULAR SUBCLUSTERS ------------------------------------------------------------------- '''\n",
    "\n",
    "\n",
    "    if n_subclu > 0:\n",
    "        scl_centroids = []\n",
    "        scl_labels = []\n",
    "        scl_ncentroids =[]\n",
    "        idx_centroids_sc= pd.DataFrame() #va almacenar los centroides de los subclusters\n",
    "        #obtener los resultados del clusterizado\n",
    "        for i in range(n_clusters):\n",
    "            if subclusters_data=='wind':\n",
    "                dfclvvnoNA=dfclvv['C'+str(i+1)].dropna()\n",
    "                #buscando centroides con los subclusters\n",
    "                kmeans_sc = cluster.KMeans(n_clusters=n_subclu, random_state=0).fit(dfclvvnoNA)\n",
    "                min_dist_sc = np.min(cdist(dfclvvnoNA.values, kmeans_sc.cluster_centers_, 'euclidean'), axis=1)\n",
    "                Y_sc = pd.DataFrame(min_dist_sc, index=dfclvvnoNA.index, columns=['PCTimeStamp'])\n",
    "                Z_sc = pd.DataFrame(kmeans_sc.labels_, index=dfclvvnoNA.index, columns=['subcluster_ID'])\n",
    "\n",
    "            elif subclusters_data=='pow':\n",
    "                dfclvpnoNA=dfclvp['C'+str(i+1)].dropna()\n",
    "                #buscando centroides con los subclusters\n",
    "                kmeans_sc = cluster.KMeans(n_clusters=n_subclu, random_state=0).fit(dfclvpnoNA)\n",
    "                min_dist_sc = np.min(cdist(dfclvpnoNA.values, kmeans_sc.cluster_centers_, 'euclidean'), axis=1)\n",
    "                Y_sc = pd.DataFrame(min_dist_sc, index=dfclvpnoNA.index, columns=['PCTimeStamp'])\n",
    "                Z_sc = pd.DataFrame(kmeans_sc.labels_, index=dfclvpnoNA.index, columns=['subcluster_ID'])\n",
    "            else:\n",
    "                print('Todavía no programado, haciendo subclusters en potencia')\n",
    "                kmeans_sc = cluster.KMeans(n_clusters=n_subclu, random_state=0).fit(dfclvp['C'+str(i+1)].dropna())\n",
    "\n",
    "            scl_centroids.append(kmeans_sc.cluster_centers_)\n",
    "            scl_labels.append(kmeans_sc.labels_)\n",
    "            scl_ncentroids.append(len(scl_centroids[i]))\n",
    "\n",
    "\n",
    "            PAP_sc = pd.concat([Y_sc,Z_sc], axis=1)\n",
    "            #poniendo index a los centroides de los subclusters\n",
    "            grouped_sc = PAP_sc.groupby(['subcluster_ID'])\n",
    "            idx_cent_sc = grouped_sc.idxmin()\n",
    "            idx_cent_sc.index +=1\n",
    "            #agregando columnas de velocidadd de viento y potencia al dataframe de los centroides\n",
    "            vxvy_sc=dfvxvy.loc[idx_cent_sc.PCTimeStamp].values\n",
    "            vvpot_sc = dfVP.loc[idx_cent_sc.PCTimeStamp].values\n",
    "            idx_centroids_sc=pd.concat([idx_centroids_sc, idx_cent_sc.assign(vx=vxvy_sc[:,0],vy=vxvy_sc[:,1],\n",
    "                                                                             vwind=vvpot_sc[:,0],\n",
    "                                                                             pw=vvpot_sc[:,1], cluster_ID='C'+str(i+1))] )\n",
    "        #creando el multiindex fuera del ciclo for\n",
    "        # idx_centroids_sc.set_index(['cluster_ID',idx_centroids_sc.index],inplace=True)\n",
    "\n",
    "        idx_centroids_sc.reset_index(inplace=True)\n",
    "        idx_centroids_sc.sort_values(['cluster_ID','vwind'], ascending=[1,1],inplace=True)\n",
    "        lst_num_sc=list(range(1,n_subclu+1))*n_clusters#crea una lista de numeros para el subcluster [1,2,3..1,2,3]\n",
    "        sc_idx_list =list(map(lambda x:'SC'+str(x),lst_num_sc))#le pone las letras SC [SC1,SC2,SC3...SC1,SC2,SC3]\n",
    "        idx_centroids_sc=idx_centroids_sc.assign(subcluster_ID_ord=sc_idx_list)\n",
    "        idx_centroids_sc.set_index(['cluster_ID','subcluster_ID_ord'],inplace=True)\n",
    "\n",
    "        ############  ORDENAR CENTROIDES:  ordenar el orden de aparicion segun la magnitud de la vv\n",
    "        scl_magni = np.zeros([n_clusters,n_subclu])\n",
    "        scl_ord = []\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "\n",
    "            for j in range(n_subclu):\n",
    "\n",
    "                vx = dfclvv['C' + str(i+1)].vx.dropna().values[scl_labels[i] == j]\n",
    "                vy = dfclvv['C' + str(i+1)].vy.dropna().values[scl_labels[i] == j]\n",
    "                scl_magni[i][j] = np.mean(np.sqrt(vx**2 + vy**2))  #magnitud de la vv\n",
    "\n",
    "            scl_ord.append( scl_magni[i].argsort())# ver https://github.com/numpy/numpy/issues/8757\n",
    "\n",
    "        #ORDENAR CENTROIDES\n",
    "        for i in range(len(scl_centroids)):\n",
    "            scl_centroids[i] = scl_centroids[i][scl_ord[i]]\n",
    "\n",
    "        #CLUSTER VIENTO POTENCIA:\n",
    "        colheadvp = []\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(n_subclu):\n",
    "                colheadvp.append(('C' + str(i + 1),'SC' + str(j + 1), 'vViento'))\n",
    "                colheadvp.append(('C' + str(i + 1),'SC' + str(j + 1), 'Pw'))\n",
    "        dfsclvp = pd.DataFrame()\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(n_subclu):\n",
    "                dfsclvp = pd.concat([\n",
    "                    dfsclvp, dfclvp['C'+str(i+1)].dropna().vViento[scl_labels[i]  == scl_ord[i][j]],\n",
    "                    dfclvp['C'+str(i+1)].dropna().Pw[scl_labels[i] == scl_ord[i][j]]\n",
    "                ],\n",
    "                    axis=1,\n",
    "                    ignore_index=True)\n",
    "        #crear multiindice\n",
    "        dfsclvp.columns = pd.MultiIndex.from_tuples(colheadvp)\n",
    "\n",
    "        #CLUSTER VX,VY:####################################################\n",
    "        colheadvv = []\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(n_subclu):\n",
    "                colheadvv.append(('C' + str(i + 1),'SC' + str(j + 1), 'vx'))\n",
    "                colheadvv.append(('C' + str(i + 1),'SC' + str(j + 1), 'vy'))\n",
    "        dfsclvv = pd.DataFrame()\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(n_subclu):\n",
    "                dfsclvv = pd.concat([\n",
    "                    dfsclvv, dfclvv['C'+str(i+1)].dropna().vx[scl_labels[i]  == scl_ord[i][j]],\n",
    "                    dfclvv['C'+str(i+1)].dropna().vy[scl_labels[i] == scl_ord[i][j]]\n",
    "                ],\n",
    "                    axis=1,\n",
    "                    ignore_index=True)\n",
    "        #crear multiindice\n",
    "        dfsclvv.columns = pd.MultiIndex.from_tuples(colheadvv)\n",
    "\n",
    "        #CLUSTER POTENCIA:\n",
    "        dfsclpw = pd.DataFrame()\n",
    "        colheadpw =[]\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(n_subclu):\n",
    "                colheadpw.append(('C' + str(i + 1),'SC' + str(j + 1)))\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            for j in range(n_subclu):\n",
    "                dfsclpw = pd.concat([dfsclpw, dfclvp['C'+str(i+1)].Pw.dropna()[scl_labels[i] == scl_ord[i][j]]],\n",
    "                                    ignore_index=True,\n",
    "                                    axis=1)\n",
    "        dfsclpw.columns = pd.MultiIndex.from_tuples(colheadpw)\n",
    "\n",
    "        #### buscar centroides para la grafica vv\n",
    "        #obtener nombre de niveles\n",
    "        n_subclu = len(dfsclvv.columns.levels[1])\n",
    "        #numero total de clusters incluidos los subclusters\n",
    "        n_tot_clusters = n_subclu*n_clusters\n",
    "        # solo aplica cuando hay subclusters\n",
    "        lev0 = dfsclvv.columns.get_level_values(0)\n",
    "        lev1 = dfsclvv.columns.get_level_values(1)\n",
    "        namcl = [(lev0[i],lev1[i]) for i in range(len(lev0))]\n",
    "        colnames = namcl[::2]\n",
    "\n",
    "\n",
    "        return  dfsclvv,dfsclpw,dfsclvp,cl_ord,kmeans.cluster_centers_   , idx_centroids,scl_ord,scl_centroids,idx_centroids_sc\n",
    "\n",
    "    else:# sin subclusters\n",
    "        scl_ord=[]\n",
    "        scl_centroids=[]\n",
    "        idx_centroids_sc=[]\n",
    "        if dfvp_is_empty:\n",
    "            return dfclvv,[],[],cl_ord,kmeans.cluster_centers_,idx_centroids,scl_ord,scl_centroids, idx_centroids_sc\n",
    "        else:\n",
    "            return dfclvv,dfclpw,dfclvp,cl_ord,kmeans.cluster_centers_,idx_centroids,scl_ord,scl_centroids, idx_centroids_sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Eliminar clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "''' FUNCION ACTUALIZADA A AGOSTO VER: https://pandas-docs.github.io/pandas-docs-travis/user_guide/advanced.html\n",
    "La palabra labels cambio a codes\n",
    "Changed in version 0.24.0: MultiIndex.labels has been renamed to MultiIndex.codes and MultiIndex.set_labels\n",
    "to MultiIndex.set_codes.\n",
    "'''\n",
    "'''TODO: POR ALGUNA RAZON NO PUEDO EJECUTAR LA MISMA FUNCION DOS VECES. ES COMO SI AFECTARA MIS VARIABLES ORIGINALES'''\n",
    "def del_clusters(data,cltodel,idx_cent,idx_cent_sc=None,scl_ord=None,cl_type='cluster'):\n",
    "    \"\"\"\n",
    "    Elimina clusters, subclusters, centroides de clusters o centroides de subclusters.\n",
    "        Argumentos:\n",
    "            dataf: dataframes a los que se eliminaran los clusters o subclusters.\n",
    "            cltodel: lista o listas de clusters o subclusters a eliminar\n",
    "            sc_centroids: lista que contiene los centroides. De esta lista se eliminaran los centroides\n",
    "                            a partir de cltodel.\n",
    "    \"\"\"\n",
    "    dflist =[]\n",
    "    for df in data:\n",
    "        dflist.append(df.drop(cltodel,axis=1))\n",
    "    #cambiar nombre por numero de centroides\n",
    "    n_cl=len(df.columns.levels[0])\n",
    "    n_subcl = len(df.columns.levels[1])\n",
    "    #numero total de clusters incluidos los subclusters\n",
    "    #sn_tot_clusters = (n_subcl*n_cl) -(len(df.columns.labels[0])-len(dflist[0].columns.labels[0]))/2\n",
    "    #sn_tot_clusters = (n_subcl*n_cl) -(len(df.columns.codes[0])-len(dflist[0].columns.codes[0]))/2\n",
    "    #eliminar centroides de clusters\n",
    "    clnames = pd.unique(df.columns.get_level_values(0))\n",
    "    lev0 = data[0].columns.get_level_values(0)\n",
    "    lev1 = data[0].columns.get_level_values(1)\n",
    "    namcl = [(lev0[i],lev1[i]) for i in range(len(lev0))]\n",
    "    cl_sc_names = namcl[::2]\n",
    "    #convertir a conjuntos\n",
    "    a =set(cltodel)# clusters a eliminar\n",
    "    b=set(cl_sc_names)#nombre de todos los clusters\n",
    "    c=(b-a)#a b lequito a\n",
    "    cent_restantes =set(sorted(set(int(item[0][1:]) for item in c)))#quito la letra C\n",
    "    idx_cent_reales = set(idx_cent.index.values)\n",
    "    idx_cent_todel = list(idx_cent_reales-cent_restantes)\n",
    "    idx_centroids_clean=idx_cent.drop(idx_cent_todel)\n",
    "\n",
    "    #eliminar centroides de los subclusters    \n",
    "    idx_centroidssc_clean = idx_cent_sc.drop(cltodel)\n",
    "  \n",
    "    return dflist,idx_centroids_clean,idx_centroidssc_clean\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# clusters a datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 3, 1, 17, 22, 53, 439969)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clust_to_data(dfv,dfvp):\n",
    "    \"\"\"\n",
    "    Convierte el dataframe de los datos de vx,vy y potencia separados en clusters (C1-SC1-vx,vy) a datos con\n",
    "    con tres columnas time,vx,vy o time,vv,pw.\n",
    "    To-do:   Sigue perdiendo miles de registros, ¿porque hay registros na en el original?\n",
    "    hay filas con solo nan en el original, en una prueba fueron como 4000\"\"\"\n",
    "    dfv= dfv.copy()#para que no afecte el original, si lo afecta el que esta como argumento\n",
    "    dfvp= dfvp.copy()\n",
    "    #Quitar multiindex\n",
    "    dfv.columns = dfv.columns.map(''.join)\n",
    "    dfv.reset_index(inplace=True)\n",
    "    #list comprehension\n",
    "    lvv=[\n",
    "       dfv.iloc[i].dropna().values\n",
    "       for i in range(len(dfv) )      \n",
    "      ]\n",
    "    dfvxvy_clean=pd.DataFrame(lvv,columns=['PCTimeStamp','vx','vy'])\n",
    "    dfvxvy_clean.dropna(inplace=True)#hay filas de nan y se pierden miles de datos, revisar\n",
    "    \n",
    "    #Quitar multiindex\n",
    "    dfvp.columns = dfvp.columns.map(''.join)\n",
    "    dfvp.reset_index(inplace=True)\n",
    "    #list comprehension\n",
    "    lvp=[\n",
    "       dfvp.iloc[i].dropna().values\n",
    "       for i in range(len(dfvp) )      \n",
    "      ]\n",
    "    dfvp_clean=pd.DataFrame(lvp,columns=['PCTimeStamp','vViento','Pw'])\n",
    "    dfvp_clean.dropna(inplace=True)#hay filas de nan y se pierden miles de datos, revisar\n",
    "    \n",
    "    return dfvxvy_clean,dfvp_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# daymin2date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "def daymin2date(year,day,hour_min):\n",
    "    \"\"\"\n",
    "\n",
    "    :param year: year when data was measured\n",
    "    :param day: day\n",
    "    :param hour_min: hour and minute in 24 hour format without separating character, e.g. 16:37 --> 1637\n",
    "    :return: timestamp in format dd/mm/YYYY HH:MM\n",
    "    \"\"\"\n",
    "    hourmin =hour_min.zfill(4)\n",
    "    hour = hourmin[:2]\n",
    "    minute = hourmin[2:]\n",
    "    res = datetime.datetime.strptime(year + \"-\" + day +\" \"+hour +\":\"+minute, \"%Y-%j %H:%M\").strftime(\"%d/%m/%Y %H:%M\")\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class kmData"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO:\n",
    "-en el idx_centroids los numeros de los clusters no llevan la C, en el idx_centroids_sc si\n",
    "    la llevan, o cambiar todo a que la lleven o que no la lleven\n",
    "-Cambiar assign, o ver como puedo poner un variable como nombre, que en lugar de pw\n",
    "    sea pw_col_name\n",
    "'''\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class KMData:\n",
    "    \"\"\"\n",
    "    Clase para tener un objeto con todos los datos de los clusters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.kmeans_labels = None\n",
    "        self.dfclvv = None\n",
    "        self.cl_ord = None\n",
    "        self.scl_ord= None\n",
    "        self.scl_centroids = None\n",
    "        self.idx_centroids = None\n",
    "        self.idx_centroids_sc = None\n",
    "        self.cl_centers = None\n",
    "        self.dfclpw = None\n",
    "        self.dfclvp= None\n",
    "\n",
    "    #crear clusters a partir de dataframes\n",
    "    def dataframe_to_cluster(self,dfvxvy, n_clusters,dfVP =None, n_sub_clusters=0, clusters_data=None,\n",
    "                             subclusters_data=None, datadir=None,pw_col_name='pw', wind_col_name='vwind'):\n",
    "        \"\"\"\n",
    "            Descripcion:\n",
    "            Esta funcion toma diferentes dataframes de entrada agrupa los datos por cluster.\n",
    "\n",
    "            :param wind_col_name: nombre de la columna \"viento\" del dataframe\n",
    "            :param pw_col_name: nombre de la columna \"potencia\" del dataframe\n",
    "            :param datadir: Datos de direccion\n",
    "            :param subclusters_data: data to do subclustering [wind,pow,dir]\n",
    "            :param clusters_data: data to do clustering [wind,pow,dir]\n",
    "            :param dfvxvy: dataframe con columnas vx y vy\n",
    "            :param dfVP: dataframe con columnas de magnitud de viento y potencia\n",
    "            :param n_clusters: numero de clusters\n",
    "            :param n_sub_clusters: Número de suclusters a calcular a partir de los n clusters calculados en un principio.\n",
    "                      Si n_sub_clusters=0 no se calcula ningun subcluster. Por defecto n_sub=0\n",
    "\n",
    "\n",
    "            :return cl_ord: un array que contiene el número de cluster n ordenado de menor a mayor\n",
    "                de las magnitudes de la velocidad de viento sin tomar en cuenta la direccion\n",
    "            :return dfclvv: dataframe donde las componentes de velocidad de viento vx y vy estan\n",
    "                agrupadas por cluster\n",
    "            :return dfclpw: dataframe donde la potencia esta agrupada por cluster\n",
    "            :return dfclvp: dataframe donde la potencia y la magnitud de viento esta agrupada por\n",
    "                cluster\n",
    "        \"\"\"\n",
    "\n",
    "        #inicializacion de variables internas\n",
    "        dfclpw = None\n",
    "        dfclvp = None\n",
    "        scl_ord = None\n",
    "        scl_centroids = None\n",
    "        idx_centroids_sc = None\n",
    "\n",
    "        #check if dataframe with power values exist\n",
    "        if dfVP is None:\n",
    "            dfvp_is_empty = True\n",
    "        else:\n",
    "            dfvp_is_empty = False\n",
    "\n",
    "        #'''- KMEANS -----------------------------------------------------------------------------------------'''\n",
    "        if clusters_data=='wind':\n",
    "            kmeans = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(dfvxvy)\n",
    "            #buscar centroides en dataframes\n",
    "            #https://stackoverflow.com/questions/42583995/get-the-centroid-row-index-from-k-means-clustering-using-sklearn\n",
    "            min_dist = np.min(cdist(dfvxvy.values, kmeans.cluster_centers_, 'euclidean'), axis=1)#distancia minima a cada centroide\n",
    "            Y = pd.DataFrame(min_dist, index=dfvxvy.index, columns=['PCTimeStamp'])\n",
    "            Z = pd.DataFrame(kmeans.labels_, index=dfvxvy.index, columns=['cluster_ID'])\n",
    "            #crea el dataframe\n",
    "            PAP = pd.concat([Y,Z], axis=1)\n",
    "\n",
    "        if clusters_data=='pow' and not dfvp_is_empty:\n",
    "\n",
    "            kmeans = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(dfVP)\n",
    "            #buscar centroides en dataframes\n",
    "            min_dist = np.min(cdist(dfVP.values, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "            Y = pd.DataFrame(min_dist, index=dfVP.index, columns=['PCTimeStamp'])\n",
    "            Z = pd.DataFrame(kmeans.labels_, index=dfVP.index, columns=['cluster_ID'])\n",
    "            PAP = pd.concat([Y,Z], axis=1)\n",
    "\n",
    "        elif clusters_data=='pow' and  dfvp_is_empty:\n",
    "\n",
    "            print('Power data is emtpy...')\n",
    "            return None\n",
    "\n",
    "        if clusters_data=='dir' and not dfvp_is_empty:\n",
    "            kmeans = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(datadir)\n",
    "            #buscar centroides en dataframes\n",
    "            min_dist = np.min(cdist(dfVP.values, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "            Y = pd.DataFrame(min_dist, index=dfVP.index, columns=['PCTimeStamp'])\n",
    "            Z = pd.DataFrame(kmeans.labels_, index=dfVP.index, columns=['cluster_ID'])\n",
    "            PAP = pd.concat([Y,Z], axis=1)\n",
    "\n",
    "        elif clusters_data =='dir' and dfvp_is_empty:\n",
    "\n",
    "            print('Clustering with only direction and no power data not implemented yet.')\n",
    "            return None\n",
    "\n",
    "        #ordend e los cluster por magnitud de vv\n",
    "        cl_magni = np.zeros(n_clusters)\n",
    "        #esta magnitud no esta correcta, funciona para ordenar\n",
    "        #pero no es la real, revisar porque no sale lo mismo si\n",
    "        #se hace los centroides. La media no es lo mismo que los\n",
    "        #centroides de kmeans?, probablemente no\n",
    "        for i in range(n_clusters):\n",
    "            #vx = dfvxvy.vx.values[kmeans.labels_ == i]\n",
    "            #vy = dfvxvy.vy.values[kmeans.labels_ == i]\n",
    "            #cl_magni[i] = np.mean(np.sqrt(vx**2 + vy**2))  #magnitud de la vv\n",
    "            cl_magni[i]=np.sqrt(\n",
    "                dfvxvy[kmeans.labels_ == i].vx.mean() **2 +\n",
    "                dfvxvy[kmeans.labels_ == i].vy.mean() **2)\n",
    "\n",
    "        cl_ord = np.argsort(cl_magni.argsort()) #ver https://github.com/numpy/numpy/issues/8757\n",
    "\n",
    "        grouped = PAP.groupby(['cluster_ID'])#agrupa por numero de clusters las distancias minimas\n",
    "        idx_centroids = grouped.idxmin()#encuentra el indice de la distanciam minima\n",
    "        #agregando columnas de velocidadd de viento y potencia al dataframe de los centroides\n",
    "        vxvy=dfvxvy.loc[idx_centroids.PCTimeStamp].values\n",
    "        if dfvp_is_empty:\n",
    "            idx_centroids=idx_centroids.assign(vx=vxvy[:,0],vy=vxvy[:,1])\n",
    "\n",
    "        else:\n",
    "            vvpot = dfVP.loc[idx_centroids.PCTimeStamp].values\n",
    "            idx_centroids=idx_centroids.assign(vx=vxvy[:,0],vy=vxvy[:,1],vwind=vvpot[:,0],pw=vvpot[:,1])\n",
    "            #tal vez se necesite si no funciona cuando le meto el plot de potencia\n",
    "            # idx_centroids.sort_values(by='vwind',inplace=True)# ordenar por velocidad de viento\n",
    "            # idx_centroids.reset_index(inplace=True) #que ya no sea el cluster id el indice\n",
    "            # idx_centroids.index.set_names('cluster_ID_ord',inplace=True)  #ponerle el nombre al indice ordenado\n",
    "            # idx_centroids.index+=1 #paraque el indice empieze en uno y no exista cluster 0\n",
    "\n",
    "        idx_centroids.reset_index(inplace=True)\n",
    "        idx_centroids['cluster_ID_ord']=cl_ord+1\n",
    "        idx_centroids.set_index('cluster_ID_ord',inplace=True)\n",
    "        idx_centroids.sort_index(inplace=True)\n",
    "        #agregar columnas de magnitud de viento\n",
    "        mag_windv = []\n",
    "        for i  in range(len(idx_centroids)):\n",
    "            mag_windv.append(np.sqrt(idx_centroids.iloc[i].vx**2 +\n",
    "                         idx_centroids.iloc[i].vy**2))\n",
    "        idx_centroids['mag_windv'] = mag_windv\n",
    "\n",
    "\n",
    "        ## CREAR MULTIINDICE ---------------------------------------------------------------'''\n",
    "        #CLUSTER VX,VY:\n",
    "        #https://stackoverflow.com/questions/37835508/how-to-do-multi-column-from-tuples\n",
    "        #nombre de las columnas del dataframe\n",
    "        if dfvp_is_empty:\n",
    "            colheadvv = []\n",
    "            for i in range(n_clusters):\n",
    "                colheadvv.append(('C' + str(i + 1), 'vx'))\n",
    "                colheadvv.append(('C' + str(i + 1), 'vy'))\n",
    "            dfclvv = pd.DataFrame()\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "                dfclvv = pd.concat([\n",
    "                    dfclvv, dfvxvy.vx[kmeans.labels_ ==np.where(cl_ord ==i)[0][0]],\n",
    "                    dfvxvy.vy[kmeans.labels_ == np.where(cl_ord ==i)[0][0]]\n",
    "                ],\n",
    "                    axis=1,\n",
    "                    ignore_index=True)\n",
    "            #crear multiindice\n",
    "            dfclvv.columns = pd.MultiIndex.from_tuples(colheadvv)\n",
    "\n",
    "        else:\n",
    "\n",
    "            colheadvv = []\n",
    "            colheadpw = []\n",
    "            for i in range(n_clusters):\n",
    "                colheadvv.append(('C' + str(i + 1), 'vx'))\n",
    "                colheadvv.append(('C' + str(i + 1), 'vy'))\n",
    "                colheadpw.append('C' + str(i + 1))\n",
    "            dfclvv = pd.DataFrame()\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "                dfclvv = pd.concat([\n",
    "                    dfclvv, dfvxvy.vx[kmeans.labels_ ==np.where(cl_ord ==i)[0][0]],\n",
    "                    dfvxvy.vy[kmeans.labels_ == np.where(cl_ord ==i)[0][0]]\n",
    "                ],\n",
    "                    axis=1,\n",
    "                    ignore_index=True)\n",
    "            #crear multiindice\n",
    "            dfclvv.columns = pd.MultiIndex.from_tuples(colheadvv)\n",
    "\n",
    "            #CLUSTER POTENCIA:\n",
    "            dfclpw = pd.DataFrame()\n",
    "            for i in range(n_clusters):\n",
    "                dfclpw = pd.concat([dfclpw, dfVP[pw_col_name][kmeans.labels_ == np.where(cl_ord ==i)[0][0]]],\n",
    "                                   ignore_index=True,\n",
    "                                   axis=1)\n",
    "            dfclpw.columns = colheadpw\n",
    "\n",
    "            #CLUSTER VIENTO POTENCIA:\n",
    "\n",
    "            colheadvp = []\n",
    "            for i in range(n_clusters):\n",
    "                colheadvp.append(('C' + str(i + 1), 'vwind'))\n",
    "                colheadvp.append(('C' + str(i + 1), 'pw'))\n",
    "            dfclvp = pd.DataFrame()\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "                dfclvp = pd.concat([\n",
    "                    dfclvp, dfVP[wind_col_name][kmeans.labels_ == np.where(cl_ord ==i)[0][0]],\n",
    "                    dfVP[pw_col_name][kmeans.labels_ ==np.where(cl_ord ==i)[0][0]]] ,\n",
    "                    axis=1,\n",
    "                    ignore_index=True)\n",
    "            #crear multiindice\n",
    "            dfclvp.columns = pd.MultiIndex.from_tuples(colheadvp)\n",
    "\n",
    "        if clusters_data=='direccion':\n",
    "            #CLUSTER direccion viento\n",
    "            colheadvp = []\n",
    "            for i in range(n_clusters):\n",
    "                colheadvp.append(('C' + str(i + 1), 'Dir'))\n",
    "                colheadvp.append(('C' + str(i + 1), pw_col_name))\n",
    "            dfcldv = pd.DataFrame()\n",
    "            for i in range(n_clusters):\n",
    "                dfcldv = pd.concat([\n",
    "                    dfcldv, datadir[wind_col_name][kmeans.labels_ ==( cl_ord ==i)],\n",
    "                    datadir.Dir[kmeans.labels_ == (cl_ord ==i)]],\n",
    "                    axis=1,ignore_index=True)\n",
    "            #crear multiindice\n",
    "            dfcldv.columns = pd.MultiIndex.from_tuples(colheadvp)\n",
    "        '''- CALCULAR SUBCLUSTERS ------------------------------------------------------------------- '''\n",
    "        if n_sub_clusters > 0:\n",
    "            scl_centroids = []\n",
    "            scl_labels = []\n",
    "            scl_ncentroids =[]\n",
    "            idx_centroids_sc= pd.DataFrame() #va almacenar los centroides de los subclusters\n",
    "            #obtener los resultados del clusterizado\n",
    "            for i in range(n_clusters):\n",
    "                if subclusters_data=='wind':\n",
    "                    dfclvvnoNA=dfclvv['C'+str(i+1)].dropna()\n",
    "                    #buscando centroides con los subclusters\n",
    "                    kmeans_sc = cluster.KMeans(n_clusters=n_sub_clusters, random_state=0).fit(dfclvvnoNA)\n",
    "                    min_dist_sc = np.min(cdist(dfclvvnoNA.values, kmeans_sc.cluster_centers_, 'euclidean'), axis=1)\n",
    "                    Y_sc = pd.DataFrame(min_dist_sc, index=dfclvvnoNA.index, columns=['PCTimeStamp'])\n",
    "                    Z_sc = pd.DataFrame(kmeans_sc.labels_, index=dfclvvnoNA.index, columns=['subcluster_ID'])\n",
    "\n",
    "                elif subclusters_data=='pow':\n",
    "                    dfclvpnoNA=dfclvp['C'+str(i+1)].dropna()\n",
    "                    #buscando centroides con los subclusters\n",
    "                    kmeans_sc = cluster.KMeans(n_clusters=n_sub_clusters, random_state=0).fit(dfclvpnoNA)\n",
    "                    min_dist_sc = np.min(cdist(dfclvpnoNA.values, kmeans_sc.cluster_centers_, 'euclidean'), axis=1)\n",
    "                    Y_sc = pd.DataFrame(min_dist_sc, index=dfclvpnoNA.index, columns=['PCTimeStamp'])\n",
    "                    Z_sc = pd.DataFrame(kmeans_sc.labels_, index=dfclvpnoNA.index, columns=['subcluster_ID'])\n",
    "                else:\n",
    "                    print('Todavía no programado, haciendo subclusters en potencia')\n",
    "                    kmeans_sc = cluster.KMeans(n_clusters=n_sub_clusters, random_state=0).fit(dfclvp['C'+str(i+1)].dropna())\n",
    "\n",
    "                scl_centroids.append(kmeans_sc.cluster_centers_)\n",
    "                scl_labels.append(kmeans_sc.labels_)\n",
    "                scl_ncentroids.append(len(scl_centroids[i]))\n",
    "\n",
    "\n",
    "                PAP_sc = pd.concat([Y_sc,Z_sc], axis=1)\n",
    "                #poniendo index a los centroides de los subclusters\n",
    "                grouped_sc = PAP_sc.groupby(['subcluster_ID'])\n",
    "                idx_cent_sc = grouped_sc.idxmin()\n",
    "                idx_cent_sc.index +=1\n",
    "                #agregando columnas de velocidadd de viento y potencia al dataframe de los centroides\n",
    "                vxvy_sc=dfvxvy.loc[idx_cent_sc.PCTimeStamp].values\n",
    "                vvpot_sc = dfVP.loc[idx_cent_sc.PCTimeStamp].values\n",
    "                idx_centroids_sc=pd.concat([idx_centroids_sc, idx_cent_sc.assign(vx=vxvy_sc[:,0],vy=vxvy_sc[:,1],vwind=vvpot_sc[:,0], pw=vvpot_sc[:,1], cluster_ID='C'+str(i+1))] )\n",
    "            #creando el multiindex fuera del ciclo for\n",
    "            # idx_centroids_sc.set_index(['cluster_ID',idx_centroids_sc.index],inplace=True)\n",
    "\n",
    "            idx_centroids_sc.reset_index(inplace=True)\n",
    "            idx_centroids_sc.sort_values(['cluster_ID',pw_col_name], ascending=[1,1],inplace=True)\n",
    "            lst_num_sc=list(range(1,n_sub_clusters+1))*n_clusters#crea una lista de numeros para el subcluster [1,2,3..1,2,3]\n",
    "            sc_idx_list =list(map(lambda x:'SC'+str(x),lst_num_sc))#le pone las letras SC [SC1,SC2,SC3...SC1,SC2,SC3]\n",
    "            idx_centroids_sc=idx_centroids_sc.assign(subcluster_ID_ord=sc_idx_list)\n",
    "            idx_centroids_sc.set_index(['cluster_ID','subcluster_ID_ord'],inplace=True)\n",
    "\n",
    "            ############  ORDENAR CENTROIDES:  ordenar el orden de aparicion segun la magnitud de la vv\n",
    "            scl_magni = np.zeros([n_clusters,n_sub_clusters])\n",
    "            scl_ord = []\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "\n",
    "                for j in range(n_sub_clusters):\n",
    "\n",
    "                    vx = dfclvv['C' + str(i+1)].vx.dropna().values[scl_labels[i] == j]\n",
    "                    vy = dfclvv['C' + str(i+1)].vy.dropna().values[scl_labels[i] == j]\n",
    "                    scl_magni[i][j] = np.mean(np.sqrt(vx**2 + vy**2))  #magnitud de la vv\n",
    "\n",
    "                scl_ord.append( scl_magni[i].argsort())# ver https://github.com/numpy/numpy/issues/8757\n",
    "\n",
    "            #ORDENAR CENTROIDES\n",
    "            for i in range(len(scl_centroids)):\n",
    "                scl_centroids[i] = scl_centroids[i][scl_ord[i]]\n",
    "\n",
    "            #CLUSTER VIENTO POTENCIA:\n",
    "            colheadvp = []\n",
    "            for i in range(n_clusters):\n",
    "                for j in range(n_sub_clusters):\n",
    "                    colheadvp.append(('C' + str(i + 1),'SC' + str(j + 1), wind_col_name))\n",
    "                    colheadvp.append(('C' + str(i + 1),'SC' + str(j + 1), pw_col_name))\n",
    "            dfsclvp = pd.DataFrame()\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "                for j in range(n_sub_clusters):\n",
    "                    dfsclvp = pd.concat([\n",
    "                        dfsclvp, dfclvp['C'+str(i+1)].dropna()[wind_col_name][scl_labels[i]  == scl_ord[i][j]],\n",
    "                        dfclvp['C'+str(i+1)].dropna()[pw_col_name][scl_labels[i] == scl_ord[i][j]]\n",
    "                    ],\n",
    "                        axis=1,\n",
    "                        ignore_index=True)\n",
    "            #crear multiindice\n",
    "            dfsclvp.columns = pd.MultiIndex.from_tuples(colheadvp)\n",
    "\n",
    "            #CLUSTER VX,VY:####################################################\n",
    "            colheadvv = []\n",
    "            for i in range(n_clusters):\n",
    "                for j in range(n_sub_clusters):\n",
    "                    colheadvv.append(('C' + str(i + 1),'SC' + str(j + 1), 'vx'))\n",
    "                    colheadvv.append(('C' + str(i + 1),'SC' + str(j + 1), 'vy'))\n",
    "            dfsclvv = pd.DataFrame()\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "                for j in range(n_sub_clusters):\n",
    "                    dfsclvv = pd.concat([\n",
    "                        dfsclvv, dfclvv['C'+str(i+1)].dropna().vx[scl_labels[i]  == scl_ord[i][j]],\n",
    "                        dfclvv['C'+str(i+1)].dropna().vy[scl_labels[i] == scl_ord[i][j]]\n",
    "                    ],\n",
    "                        axis=1,\n",
    "                        ignore_index=True)\n",
    "            #crear multiindice\n",
    "            dfsclvv.columns = pd.MultiIndex.from_tuples(colheadvv)\n",
    "\n",
    "            #CLUSTER POTENCIA:\n",
    "            dfsclpw = pd.DataFrame()\n",
    "            colheadpw =[]\n",
    "            for i in range(n_clusters):\n",
    "                for j in range(n_sub_clusters):\n",
    "                    colheadpw.append(('C' + str(i + 1),'SC' + str(j + 1)))\n",
    "\n",
    "            for i in range(n_clusters):\n",
    "                for j in range(n_sub_clusters):\n",
    "                    dfsclpw = pd.concat([dfsclpw,\n",
    "                                         dfclvp['C'+str(i+1)][pw_col_name].dropna()[scl_labels[i] ==scl_ord[i][j]]],\n",
    "                                        ignore_index=True,\n",
    "                                        axis=1)\n",
    "            dfsclpw.columns = pd.MultiIndex.from_tuples(colheadpw)\n",
    "\n",
    "            #### buscar centroides para la grafica vv\n",
    "            #obtener nombre de niveles\n",
    "            n_sub_clusters = len(dfsclvv.columns.levels[1])\n",
    "            #numero total de clusters incluidos los subclusters\n",
    "            n_tot_clusters = n_sub_clusters*n_clusters\n",
    "            # solo aplica cuando hay subclusters\n",
    "            lev0 = dfsclvv.columns.get_level_values(0)\n",
    "            lev1 = dfsclvv.columns.get_level_values(1)\n",
    "            namcl = [(lev0[i],lev1[i]) for i in range(len(lev0))]\n",
    "            colnames = namcl[::2]\n",
    "\n",
    "        # llenado de datos\n",
    "        self.kmeans_labels = kmeans.labels_\n",
    "        self.comp_vel = dfclvv\n",
    "        self.cl_ord = cl_ord\n",
    "        self.idx_centroids = idx_centroids\n",
    "        self.cl_centers = kmeans.cluster_centers_\n",
    "        #self.dfclpw = dfclpw\n",
    "        self.dfclvp= dfclvp\n",
    "\n",
    "        self.scl_ord= scl_ord\n",
    "        self.scl_centroids = scl_centroids\n",
    "        self.idx_centroids_sc = idx_centroids_sc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Procesar datos de BCS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Se reemplaza la hora 24:00 por 00:00 y se suma un día, así que la última\n",
    "toma de datos de un día se cambia por la primera del día siguiente\n",
    "'''\n",
    "def proc_dat_bcs(data_path,mf_pow_curve_path=None,mf_vel_colname =None,mf_pw_colname = None, mf_rename_cols = None ):\n",
    "    \"\"\"\n",
    "    Procesamiento de datos de BCS, viento y curva del fabricante.\n",
    "    :param data_path: ruta del archivo de los datos de viento\n",
    "    :param mf_pow_curve_path: file that contains wind velocity and power of the manufacturer power curve. Can be a\n",
    "    CSV file or an Excel file\n",
    "    :param mf_vel_colname: manufacturer power curve wind velocity column name.\n",
    "    :return: mf_pw_colname: manufacturer power curve power column name.\n",
    "    \"\"\"\n",
    "    ###########################################configuración\n",
    "    columns_to_use = [0,1,3,6]\n",
    "    columns_id =['day','hour','wdir','vwind']\n",
    "    dir_format= 'deg'\n",
    "    year = '2005' #data year\n",
    "    ########################################################\n",
    "    #imprimir a consola\n",
    "    os.write(1, b\"Inciando procesamiento de datos...\\n\")\n",
    "    print(\"Inciando procesamiento de datos...\\n\")\n",
    "    dataVDxls = pd.read_excel(data_path,usecols=columns_to_use,dtype={'DIA' : str, 'HORA':str})\n",
    "    dataVDxls.columns =columns_id\n",
    "    #agrego la columna de potencia instantanea sin filtrar\n",
    "    #dataVPxls['Pw']= (dataVPxls.iloc[1:,1].values-dataVPxls.iloc[0:-1,1]) * np.pi*45**2\n",
    "    print('Total de registros: ' + str(len(dataVDxls)))\n",
    "\n",
    "    #marcando los datos faltantes asignando un nan a la fila completa\n",
    "    datamk = dataVDxls\n",
    "    datamk.loc[datamk.isnull().any(axis=1), :] = np.nan\n",
    "    #numero de filas sin datos\n",
    "    print('Numero de filas sin datos')\n",
    "    print(datamk.loc[datamk.isnull().any(axis=1), :].isnull().sum())\n",
    "\n",
    "    #eliminando filas con NaN\n",
    "    cleanData = datamk.dropna()\n",
    "\n",
    "\n",
    "    #datos direccion velocidad\n",
    "    #print(len(dataVP))\n",
    "    #print(len(dataDir))\n",
    "    #dataDV = pd.concat([dataDir,dataVP.vViento],axis=1)\n",
    "    #dataVD = pd.concat([dataVP.vViento,dataDir,axis=1)\n",
    "    #dataVcD =pd.concat([dataDir,df_comp_vel],axis=1)\n",
    "    #change hour 24:00 to 00:00\n",
    "    dataVDxls['hour']=dataVDxls['hour'].str.replace('2400','0000')\n",
    "    dataVDxls['timeStamp']= dataVDxls.apply(lambda x: daymin2date(year,x.day, x.hour), axis=1)\n",
    "    dataVDxls['timeStamp'] =pd.to_datetime(pd.to_datetime(dataVDxls['timeStamp']).dt.strftime('%d/%m/%Y %H:%M'))\n",
    "    #cambiar formato de 24:00 a 00:00 sumando un día a 00:00\n",
    "    filtro1 = dataVDxls['timeStamp'].dt.hour==0\n",
    "    filtro2 = dataVDxls['timeStamp'].dt.minute==0\n",
    "    idx =np.where(filtro1.values & filtro2.values)\n",
    "    ts = dataVDxls['timeStamp'].values\n",
    "    ts[idx]=ts[idx] + pd.to_timedelta(1,'d')\n",
    "    dataVDxls['timeStamp'] = ts\n",
    "\n",
    "    dataVDxls.set_index('timeStamp',inplace=True,verify_integrity = True)\n",
    "    dfWD = dataVDxls.drop(columns=['day','hour'])\n",
    "    #eliminar la ultima fila, ya que con el cambio de formato de hora llego\n",
    "    #al 01/01/2006 y no me interesa esa fecha\n",
    "    dfWD.drop(dfWD.tail(1).index,inplace=True) # drop last n rows\n",
    "    #dfWD.index = pd.to_datetime(pd.to_datetime(dfWD.index).strftime('%d/%m/%Y %H:%M'))\n",
    "\n",
    "    del dataVDxls\n",
    "    #CHECK THE DIRECTION OF THE ANEMOMETER\n",
    "    if dir_format == 'rad':\n",
    "        direcvrad= np.deg2rad(dfWD['wdir'].values)\n",
    "        vecVel = [-np.sin(direcvrad)*dfWD['vwind'].values,np.cos(direcvrad)*dfWD['vwind'].values]\n",
    "    else:\n",
    "        vecVel = [-np.sin(dfWD['wdir'].values * np.pi / 180) * dfWD['vwind'].values,\n",
    "                  np.cos(dfWD['wdir'].values * np.pi / 180)* dfWD['vwind'].values]\n",
    "\n",
    "\n",
    "    vecVelnp = np.array(vecVel).transpose()\n",
    "    #original sin timestamp\n",
    "    #df_comp_vel = pd.DataFrame(data=vecVelnp,columns=['vx','vy']\n",
    "    #con timestamp\n",
    "    df_comp_vel = pd.DataFrame(data=vecVelnp,columns=['vx','vy'],index=dfWD.index)\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    #               Manufacturer power curve                  -\n",
    "    #----------------------------------------------------------\n",
    "    if mf_pow_curve_path is not None:\n",
    "        if mf_pow_curve_path[-3:] == 'csv':\n",
    "            dfMfgCurve = pd.read_csv(mf_pow_curve_path)\n",
    "        else:\n",
    "            dfMfgCurve = pd.read_excel(mf_pow_curve_path,usecols=[0,2],index_col=0)\n",
    "        if mf_rename_cols is not None:\n",
    "            dfMfgCurve.rename(columns = {mf_vel_colname:mf_rename_cols[0],mf_pw_colname:mf_rename_cols[1]},\n",
    "            inplace=True)\n",
    "            dfMfgCurve.set_index(mf_rename_cols[0],inplace=True)\n",
    "        else:\n",
    "            dfMfgCurve.set_index(mf_vel_colname,inplace=True)\n",
    "\n",
    "        os.write(1, b\"Fin del procesamiento de datos\\n\")\n",
    "        print('Fin del procesamiento de datos')\n",
    "        return dfWD,df_comp_vel,dfMfgCurve\n",
    "\n",
    "    else:\n",
    "        os.write(1, b\"Fin del procesamiento de datos\\n\")\n",
    "        print('Fin del procesamiento de datos')\n",
    "        return dfWD,df_comp_vel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpolar curva del fabricante"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def interp_pow_curve(df_mf_curve,wind_min_max=(0,25),step=0.1):\n",
    "    \"\"\"\n",
    "    Interpolación de la curva del fabricante.\n",
    "    :param wind_min_max: min y max de las velocidades de viento\n",
    "    :param df_mf_curve: dataframe con los datos de vel y pw del fabricante.\n",
    "    :param step: paso entre una potencia y otra\n",
    "    :return: curva del fabricante interpolada\n",
    "    \"\"\"\n",
    "    #crear un df con datos de velocidad de viento calculadas con range\n",
    "    #y una fila pw de nan's para despues llenar esa fila con los datos de potencia\n",
    "    #aqui solo crea el df sin interpolar\n",
    "    #para el indice de vv\n",
    "    wind_range = list(np.round(np.arange(wind_min_max[0],wind_min_max[1],step),decimals=1))\n",
    "    df_mfcurve_interp = pd.DataFrame(data = {'pw': np.nan}, index =wind_range)\n",
    "    #con el indice de las velocidades de viento que existen, voy a copiar las potencias que existen\n",
    "    #al dataframe a interpolar\n",
    "    for i in df_mf_curve.index:\n",
    "        df_mfcurve_interp[df_mfcurve_interp.index == i] = df_mf_curve.loc[i].pw\n",
    "    #interpolando\n",
    "    df_mfcurve_interp.interpolate(method='linear',inplace=True )\n",
    "    #reemplazando nan con cero (viento bajo,cero potencia)\n",
    "    df_mfcurve_interp.fillna(0,inplace=True)\n",
    "\n",
    "    return df_mfcurve_interp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Crear plot de la cadena de Markov"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from matplotlib import image\n",
    "\n",
    "'''\n",
    "TO DO:\n",
    "    - el NodeFontSize no funciona para versiones antes de MATLAB 2019,\n",
    "    tal vez hacer esto: https://la.mathworks.com/matlabcentral/answers/450580-change-font-size-of-node-name-in-a-graph\n",
    "'''\n",
    "def create_mc_plot(trans_matrix,marker_size=10,node_fontsize=14,line_width =1,x_width =20,y_width=10,fig_size=(20,10)):\n",
    "    \"\"\"\n",
    "    Crea un plot con la cardena de Markov a partir del dataframe que contiene la matriz de transicion de probabilidades.\n",
    "    El dataframe debe contener los nombres de columnas en el header y en el index (es el mismo que se utiliza para\n",
    "    crear el heatmap).\n",
    "\n",
    "    Esta funcion crea un script de matlab con lo necesario para crear el plot de la cadena de Markov\n",
    "    y lo ejecuta. Este script crea una imagen en el mismo directorio del script de python llamada graph_mc.jpg\n",
    "    y lo carga en el notebook con matplotlib e image show\n",
    "\n",
    "    :param fig_size: tamaño de la figura que se muestra en el notebook.\n",
    "    :param trans_matrix: dataframe que contiene la matriz de transicion de probabilidades.\n",
    "    :param marker_size: tamaño del nodo (punto que representa al estado)\n",
    "    :param node_fontsize: tamaño de la fuente.\n",
    "    :param line_width: tamaño de linea.\n",
    "    :param x_width: ancho de la imagen en pulgadas.\n",
    "    :param y_width: largo de la imagen en pulgadas.\n",
    "    \"\"\"\n",
    "    #tamaño del nodo (punto)\n",
    "    marker_size= str(marker_size)\n",
    "    node_fontsize = str(node_fontsize)\n",
    "    line_width = str(line_width)\n",
    "    x_width=str(x_width) ;y_width=str(y_width);\n",
    "\n",
    "    #ruta y version de matlab, recordar que se necesita el econometrics toolbox\n",
    "    #y ejecutar la version de matlab donde se instalo esa toolbox, ademas\n",
    "    #de que debe ser minimo la vesion 2019 de matlab\n",
    "    matlab_path =r'\"C:\\Program Files\\MATLAB\\R2019a\\bin\\matlab.exe\"'\n",
    "    f = open(\"m_script_markov.m\", \"w\")\n",
    "    f.write('%------------------------------------------------------------------------\\n')\n",
    "    f.write('% Script con los datos de la matriz de python, que seran convertidos a un\\n')\n",
    "    f.write('% objeto dtmc de MATLAB. Luego se crea una imagen con la cadena de Markov en\\n')\n",
    "    f.write('% forma grafica y se carga en el notebook de jupyter\\n')\n",
    "    f.write('%------------------------------------------------------------------------ \\n')\n",
    "\n",
    "    #escribiendo la matriz de transicion de probabilidades en el archivo .m\n",
    "    f.write('mat_trans = [')\n",
    "    for i in range(len(trans_matrix)):\n",
    "        lin = str(trans_matrix.iloc[i].values).replace ('\\n', '')\n",
    "        f.write(lin[1:-1]+';\\n')\n",
    "    f.write('];\\n')\n",
    "    #crear el objeto discrete time markov chain\n",
    "    f.write('mc = dtmc(mat_trans);\\n')\n",
    "    #crear los nombres de los estados\n",
    "    state_names = str(trans_matrix.columns.values).replace('\\n','') +';\\n'\n",
    "    state_names =state_names.replace('\\'','\\\"')\n",
    "    f.write('mc.StateNames = ' + state_names)\n",
    "    f.write('p = graphplot(mc,\"ColorEdges\",true);\\n')\n",
    "    f.write('p.MarkerSize ='+ marker_size +';\\n')\n",
    "    f.write('p.NodeFontSize = '+ node_fontsize +';\\n')\n",
    "    f.write('p.LineWidth = ' + line_width +';\\n')\n",
    "    f.write('p.EdgeAlpha = 1;\\n')\n",
    "    f.write('colormap(\"hot\");\\n')\n",
    "    f.write('set(gcf, \"PaperUnits\", \"inches\");\\n')\n",
    "    f.write('set(gcf, \"PaperPosition\", [0 0 '+ x_width +' ' + y_width+']);\\n')\n",
    "    f.write(\"saveas(gcf,'graph_mc.jpg');\\n\")\n",
    "    f.close()\n",
    "    #corriendo archivo m en matlab por linea de comandos del sistema\n",
    "    #result almacena el texto que devuelde la terminal\n",
    "    result = subprocess.run(matlab_path + ' -batch \"m_script_markov\"', stdout=subprocess.PIPE)\n",
    "    #cargando y mostrando imagen\n",
    "    img = image.imread('graph_mc.jpg')\n",
    "\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(img,aspect='auto')\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot de direcciones de viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO:\n",
    "    - plot polar\n",
    "    - poner direcciones en los ejes\n",
    "'''\n",
    "def plot_vectors(comp_vel, show_month = None,figsize = (7,5),fign=100):\n",
    "    \"\"\"\n",
    "    Ploteo de componentes de velocidad en un plot tipo rosa de los vientos.\n",
    "    :param fign: Numero de figura.\n",
    "    :param figsize: Tamaño de la figura\n",
    "    :param show_month: Mes a mostrar, si es cero se muestran todos, si no se muestra n,\n",
    "                si es None, se muestran todos los meses en uno solo\n",
    "    :param comp_vel: dataframe con los componentes de velocidad en columnas vx y vy\n",
    "    \"\"\"\n",
    "    #https://stackoverflow.com/questions/42281966/how-to-plot-vectors-in-python-using-matplotlib\n",
    "    #para no importar mas librerias\n",
    "    months = [\"Unknown\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\"Jul\",\n",
    "              \"Aug\", \"Sep\", \"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "\n",
    "\n",
    "    f = plt.figure(fign,figsize=figsize)\n",
    "\n",
    "    limix = [np.floor(comp_vel.vx.min()),np.ceil((comp_vel.vx.max()))]\n",
    "    limiy = [np.floor(comp_vel.vy.min()),np.ceil((comp_vel.vy.max()))]\n",
    "    limir = np.ceil(np.max(comp_vel.max()))\n",
    "    if show_month is None:\n",
    "        #mostrar todos los meses en un solo plot\n",
    "        origin = np.array([np.zeros(len(comp_vel)),np.zeros(len(comp_vel))]) # origin point\n",
    "        plt.suptitle('All months')\n",
    "        plt.quiver(*origin, comp_vel.vx, comp_vel.vy, color ='b',angles='xy', scale_units='xy', scale=1)\n",
    "        plt.xlim(limix)\n",
    "        plt.ylim(limiy)\n",
    "\n",
    "    elif show_month  == 0:\n",
    "        #mostrar todos los meses en 12 plots\n",
    "        #fig, ax = plt.subplots(3,4,subplot_kw={'projection': 'polar'})\n",
    "        fig, ax = plt.subplots(3,4,num=fign)\n",
    "        counter_month = 1 #contador para la lista de meses\n",
    "        for i in range(3):\n",
    "            for j in range(4):\n",
    "                #slicing mes por mes\n",
    "                data_month = comp_vel[comp_vel.index.month==i+j+1]\n",
    "                origin = np.array([np.zeros(len(data_month)),np.zeros(len(data_month))]) # origin point\n",
    "                ax[i,j].quiver(*origin, data_month.vx, data_month.vy,\n",
    "                               color ='b',angles='xy', scale_units='xy', scale=1)\n",
    "                #ax[i,j].title.set_text(months[i+j+1])\n",
    "                ax[i,j].title.set_text(months[counter_month])\n",
    "                ax[i,j].set_xlim(limix)\n",
    "                ax[i,j].set_ylim(limiy)\n",
    "                counter_month+=1\n",
    "                #ax[i,j].set_ylim([0,np.pi])\n",
    "                #ax[i,j].set_rmax(limir)\n",
    "    elif 0 < show_month <= 12:\n",
    "        data_month = comp_vel[comp_vel.index.month==show_month]\n",
    "        origin = np.array([np.zeros(len(data_month)),np.zeros(len(data_month))]) # origin point\n",
    "        plt.suptitle(months[show_month])\n",
    "        plt.quiver(*origin, data_month.vx, data_month.vy,\n",
    "                   color ='b',angles='xy', scale_units='xy', scale=1,lw = 1)\n",
    "        plt.xlim(limix)\n",
    "        plt.ylim(limiy)\n",
    "    else:\n",
    "        print('Error. Parámetro show_month no valido.')\n",
    "    plt.figtext(0.5, 0.01, 'Figure '+ str(fign), wrap=True, horizontalalignment='center', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Combinar columnas de clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO:\n",
    "cargar una lista  con listas de clusters a unir para no tener que llamar varias veces a la funcion de unir clusters\n",
    "'''\n",
    "def join_clusters(df,clu_to_join):\n",
    "\n",
    "    \"\"\"\n",
    "    Combinas las columnas especificadas en clu_to_join en una sola, es decir, combina clusters, df = dfclvv\n",
    "    :param df: dataframe con las columnas a combinar.\n",
    "    :param clu_to_join: clusters a combinar\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if df.columns.nlevels ==1:\n",
    "        #Nombra a la columna combinada como el primer nombre de columna pasado en clu_to_join\n",
    "        #join (sumar) las columnas elegidas\n",
    "        #con min_count=1 no se suman los valores si son solo NaN y no se vuelve ese NaN cero, si no se lo pongo, los NaN\n",
    "        # se vuelven cero\n",
    "        df = df.assign(C_=df[clu_to_join].sum(1,min_count=1)).drop(clu_to_join, axis= 1)\n",
    "        #cambiar el nombre de columna temporal C_ a el primer valor de la lista de columnas a combinar\n",
    "        df.rename({'C_':clu_to_join[0]},axis=1,inplace=True)\n",
    "        #reordenar columnas alfabeticamente\n",
    "        df =df.reindex(sorted(df.columns, key=lambda x: float(x[1:])), axis=1)\n",
    "    elif df.columns.nlevels == 2:\n",
    "        #multiindice, para el df de componentes de velocidad dividido por clusters\n",
    "        #groupby agrupa todos las columnas de los clusters seleccionados en una, sum suma esas columnas y min_count\n",
    "        # hace que n + NaN sea n. Sin min_count creo que la suma da cero\n",
    "        df_ = df[clu_to_join].groupby(level=1,axis=1).sum(min_count=1)\n",
    "        #crea el nombre de las columnas\n",
    "        df_.columns = pd.MultiIndex.from_product([['C_'],['vx','vy']])\n",
    "        #une las columnas agrupadas al df de entrada\n",
    "        df = df.join(df_)\n",
    "        #elimina las columnas individuales que fueron agrupadas\n",
    "        df.drop(clu_to_join,level=0,axis=1,inplace=True)\n",
    "        #cambia el nombre de la nueva columna de agrupamientos, de C_ al primer de la lista de clusters que se pasó\n",
    "        df.rename({'C_':clu_to_join[0]},axis=1,inplace=True)\n",
    "    else:\n",
    "        print('Error: Formato de Dataframe no soportado.')\n",
    "        \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Crear plot de la cadena de Markov"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from matplotlib import image\n",
    "from os.path import exists\n",
    "\n",
    "'''\n",
    "TO DO:\n",
    "    - el NodeFontSize no funciona para versiones antes de MATLAB 2019,\n",
    "    tal vez hacer esto: https://la.mathworks.com/matlabcentral/answers/450580-change-font-size-of-node-name-in-a-graph\n",
    "    - Eliminar imagen anterior para que no se cargue cuando hay un error, o asegurarse de que no haya error al crear\n",
    "        la imagen.\n",
    "    - Cambiar la condición para cambiar cero por nan, ahorita busca el string \"0.\" pero no es lo ideal, que pasa si es\n",
    "        \"0\" o \"0e+0\"?\n",
    "'''\n",
    "\n",
    "\n",
    "def create_mc_plot(fign,trans_matrix, filename='graph_mc.jpg', marker_size=10, node_fontsize=14, line_width=1, \\\n",
    "                                                                                                         x_width=20, \\\n",
    "                                                                                                           y_width=10,\n",
    "                   fig_size=(10, 5),replace_zero_to_nan = True, as_pct = True,\n",
    "                   layout='auto'):\n",
    "    \"\"\"\n",
    "    Crea un plot con la cardena de Markov a partir del dataframe que contiene la matriz de transicion de probabilidades.\n",
    "    El dataframe debe contener los nombres de columnas en el header y en el index (es el mismo que se utiliza para\n",
    "    crear el heatmap).\n",
    "\n",
    "    Esta funcion crea un script de matlab con lo necesario para crear el plot de la cadena de Markov\n",
    "    y lo ejecuta. Este script crea una imagen en el mismo directorio del script de python llamada graph_mc.jpg\n",
    "    y lo carga en el notebook con matplotlib e image show\n",
    "\n",
    "    :param filename: Nombre del archivo de la imagen.\n",
    "    :param fign: numero de figura\n",
    "    :param layout: opciones de layout de matlab para graphplot. No todas están implementadas: UseGravity, auto, WeightEffect, layered, circle.\n",
    "    :param as_pct: si el df viene en porcentajes, dividir entre 100\n",
    "    :param replace_zero_to_nan: reemplazar ceros con np.nan\n",
    "    :param fig_size: tamaño de la figura que se muestra en el notebook.\n",
    "    :param trans_matrix: dataframe que contiene la matriz de transicion de probabilidades.\n",
    "    :param marker_size: tamaño del nodo (punto que representa al estado)\n",
    "    :param node_fontsize: tamaño de la fuente.\n",
    "    :param line_width: tamaño de linea.\n",
    "    :param x_width: ancho de la imagen en pulgadas.\n",
    "    :param y_width: largo de la imagen en pulgadas.\n",
    "    \"\"\"\n",
    "    #tamaño del nodo (punto)\n",
    "    marker_size = str(marker_size)\n",
    "    node_fontsize = str(node_fontsize)\n",
    "    line_width = str(line_width)\n",
    "    x_width = str(x_width)\n",
    "    y_width = str(y_width)\n",
    "    # si se quiere en forma de porcentaje o no\n",
    "    #revisar si el archivo existe\n",
    "    im_exists = exists('graph_mc.jpg')\n",
    "    if im_exists:\n",
    "        #si la imagen existe, borrar\n",
    "        os.remove('graph_mc.jpg')\n",
    "\n",
    "    if not as_pct:\n",
    "        trans_matrix /= 100\n",
    "\n",
    "    #ruta y version de matlab, recordar que se necesita el econometrics toolbox\n",
    "    #y ejecutar la version de matlab donde se instalo esa toolbox, ademas\n",
    "    #de que debe ser minimo la vesion 2019 de matlab\n",
    "    matlab_path = r'\"C:\\Program Files\\MATLAB\\R2019a\\bin\\matlab.exe\"'\n",
    "    f = open(\"m_script_markov.m\", \"w\")\n",
    "    f.write('%------------------------------------------------------------------------\\n')\n",
    "    f.write('% Script con los datos de la matriz de python, que seran convertidos a un\\n')\n",
    "    f.write('% objeto dtmc de MATLAB. Luego se crea una imagen con la cadena de Markov en\\n')\n",
    "    f.write('% forma grafica y se carga en el notebook de jupyter\\n')\n",
    "    f.write('%------------------------------------------------------------------------ \\n')\n",
    "\n",
    "    #escribiendo la matriz de transicion de probabilidades en el archivo .m\n",
    "    f.write('mat_trans = [')\n",
    "    if replace_zero_to_nan:\n",
    "        trans_matrix = np.round(trans_matrix,4)\n",
    "        trans_matrix.replace(0,np.nan,inplace= True)\n",
    "    for i in range(len(trans_matrix)):\n",
    "        lin = str(trans_matrix.iloc[i].values).replace('\\n', '')\n",
    "        f.write(lin[1:-1] + ';\\n')\n",
    "    f.write('];\\n')\n",
    "    #crear el objeto discrete time markov chain\n",
    "    f.write('mc = dtmc(mat_trans);\\n')\n",
    "    #crear los nombres de los estados\n",
    "    state_names = str(trans_matrix.columns.values).replace('\\n', '') + ';\\n'\n",
    "    state_names = state_names.replace('\\'', '\\\"')\n",
    "    f.write('mc.StateNames = ' + state_names)\n",
    "    f.write('p = graphplot(mc,\"ColorEdges\",true);\\n')\n",
    "    #layout del grafico\n",
    "    if layout =='auto':\n",
    "        f.write(\"layout(p,'auto');\\n\")\n",
    "    elif layout == 'WeightEffect':\n",
    "        #este layout no acepta valores cero o NaN\n",
    "        if trans_matrix.eq(0).values.any() or trans_matrix.isnull().values.any():\n",
    "            f.write(\"layout(p,'force','WeightEffect','direct');\\n\")\n",
    "        else:\n",
    "            f.write(\"layout(p,'auto');\\n\")\n",
    "    elif layout == 'circle':\n",
    "        f.write(\"layout(p,'circle');\\n\")\n",
    "    elif  layout == 'layered':\n",
    "        f.write(\"layout(p,'layered','Direction','right';)\\n\")\n",
    "    elif layout =='UseGravity':\n",
    "        f.write(\"layout(p,'force','UseGravity' ,'on');\\n\")\n",
    "    else:\n",
    "        print('Layout no reconocida, usando auto')\n",
    "        f.write(\"layout(p,'auto');\\n\")\n",
    "\n",
    "    f.write('p.MarkerSize =' + marker_size + ';\\n')\n",
    "    f.write('p.NodeFontSize = ' + node_fontsize + ';\\n')\n",
    "    f.write('p.LineWidth = ' + line_width + ';\\n')\n",
    "    f.write('p.EdgeAlpha = 1;\\n')\n",
    "    f.write('colormap(\"hot\");\\n')\n",
    "    #para saber si esta en porcentaje o no\n",
    "    #si uso solo max() toma al valor NaN como el maximo y no funciona\n",
    "    if np.nanmax(trans_matrix.values) > 1:\n",
    "        f.write('caxis([0,100]);\\n')\n",
    "    #poner los pesos a las flechas\n",
    "    f.write(\"if isempty(find(isnan(mat_trans), 1))\\n\")\n",
    "    f.write(\"   p.EdgeLabelMode= 'auto';\\n\")\n",
    "    f.write(\"else\\n\")\n",
    "    f.write(\"   p.EdgeLabelMode= 'manual';\\n\")\n",
    "    f.write(\"   idx =find(~isnan(mat_trans));\\n\")\n",
    "    f.write(\"   vals = mat_trans(idx);\\n\")\n",
    "    f.write(\"   labeledge(p,idx,vals);\\n\")\n",
    "    f.write(\"end\\n\")\n",
    "    #tamaño de la imagen\n",
    "    f.write('set(gcf, \"PaperUnits\", \"inches\");\\n')\n",
    "    f.write('set(gcf, \"PaperPosition\", [0 0 ' + x_width + ' ' + y_width + ']);\\n')\n",
    "    f.write(\"saveas(gcf,'\"+ filename +\"');\\n\")\n",
    "    f.close()\n",
    "\n",
    "    #corriendo archivo m en matlab por linea de comandos del sistema\n",
    "    #result almacena el texto que devuelde la terminal\n",
    "    result = subprocess.run(matlab_path + ' -batch \"m_script_markov\"', stdout=subprocess.PIPE)\n",
    "    #cargando y mostrando imagen\n",
    "    img = image.imread(filename)\n",
    "\n",
    "    plt.figure(fign,figsize=fig_size)\n",
    "    plt.imshow(img, aspect='auto')\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unir clusters manualmente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "#TODO: join also power\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "\n",
    "\n",
    "def join_clusters(df,clu_to_join,clu_to_del=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Combinas las columnas especificadas en clu_to_join en una sola, es decir, combina clusters, df = dfclvv. El orden\n",
    "     de los elementos de clu_to_join importa, ese seria el \"orden natural\" (segunda columna) y empieza en cero.\n",
    "     También es important usar doble corchete.\n",
    "    :param clu_to_del: Lista de cluster a eliminar\n",
    "    :param df: dataframe con las columnas a combinar. Puede ser el dataframe con clusters de solo magnitud viento\n",
    "    (dfclvv) o\n",
    "    con componentes de velocidad\n",
    "    :param clu_to_join: clusters a combinar en pares. [[1,2],[7,9] une el primero el cluster 1 y 2 y después une el\n",
    "    cluster 7,9.\n",
    "    :return: df con clusters agrupados.\n",
    "    \"\"\"\n",
    "    #revisar que clu_to_join tenga el formato [[a,b]] para un par de clusters a agrupar.\n",
    "    #mejorar como saber si estan pasando el formato adecudado\n",
    "    if len(np.array(clu_to_join,dtype='object').shape) != 1:\n",
    "        logging.error('Formato de lista de grupos errónea. Sigue el formato [[a,b...n]]?')\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    #clusters de solo magnitud de viento (dfclvv)\n",
    "    if df.columns.nlevels ==1:\n",
    "        for cl in clu_to_join:\n",
    "\n",
    "            #Nombra a la columna combinada como el primer nombre de columna pasado en clu_to_join\n",
    "            #join (sumar) las columnas elegidas\n",
    "            #con min_count=1 no se suman los valores si son solo NaN y no se vuelve ese NaN cero, si no se lo pongo, los NaN\n",
    "            # se vuelven cero\n",
    "            df = df.assign(C_=df[cl].sum(1,min_count=1)).drop(cl, axis= 1)\n",
    "            #cambiar el nombre de columna temporal C_ a el primer valor de la lista de columnas a combinar\n",
    "            df.rename({'C_':cl[0]},axis=1,inplace=True)\n",
    "            #reordenar columnas alfabeticamente\n",
    "            df =df.reindex(sorted(df.columns, key=lambda x: float(x[1:])), axis=1)\n",
    "\n",
    "\n",
    "        if clu_to_del is not None:\n",
    "            for cl in clu_to_del:\n",
    "                df.drop(cl,inplace=True,axis=1)\n",
    "    elif df.columns.nlevels == 2:\n",
    "        for cl in clu_to_join:\n",
    "            #multiindice, para el df de componentes de velocidad dividido por clusters\n",
    "            #groupby agrupa todos las columnas de los clusters seleccionados en una, sum suma esas columnas y min_count\n",
    "            # hace que n + NaN sea n. Sin min_count creo que la suma da cero\n",
    "            df_ = df[cl].groupby(level=1,axis=1).sum(min_count=1)\n",
    "            #crea el nombre de las columnas\n",
    "            df_.columns = pd.MultiIndex.from_product([['C_'],['vx','vy']])\n",
    "            #une las columnas agrupadas al df de entrada\n",
    "            df = df.join(df_)\n",
    "            #elimina las columnas individuales que fueron agrupadas\n",
    "            df.drop(cl,level=0,axis=1,inplace=True)\n",
    "            #cambia el nombre de la nueva columna de agrupamientos, de C_ al primer de la lista de clusters que se pasó\n",
    "            df.rename({'C_':cl[0]},axis=1,inplace=True)\n",
    "\n",
    "        #eliminando clusters\n",
    "        if clu_to_del is not None:\n",
    "            for cl in clu_to_del:\n",
    "                df.drop(cl,axis=1,level=0,inplace=True)\n",
    "                #si no hago esto no se quita el C7 del indice y ocurre un error al querer plotear C7 que no existe\n",
    "                df.columns =df.columns.remove_unused_levels()\n",
    "        #orden en que los clusters fueron agrupados\n",
    "        #quitando letras y dejando solo numeros\n",
    "        #orden_natural = [int(i[1:]) for i in df.columns.levels[0]]\n",
    "        #ordenar clusters por magnitud\n",
    "\n",
    "    else:\n",
    "        logging.error('Error: Formato de Dataframe no soportado.')\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Matriz de transición a partir de dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trans_matrix_from_df(df,mat_mode='prob'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param mat_mode: \"prob\" regresa la matriz de probabilidades, \"frec\" regresa la matriz de frecuencias\n",
    "    :param df: dataframe con los datos separados en clusters\n",
    "    :return: dataframe con la matriz de transiciones.\n",
    "    \"\"\"\n",
    "    #matriz de transicion del tamaño del numero de  clusters\n",
    "    if  mat_mode != 'prob' and mat_mode != 'frec':\n",
    "        print('Modo no reconocido.')\n",
    "        return None\n",
    "    n_clusters = len(df.columns)\n",
    "    mattra = np.zeros([n_clusters,n_clusters])\n",
    "    for row in range(len(df)-1):\n",
    "        #para asegurarse que no sean todos los valores nan, ahora posible cunando se eliminan clusters (joined)\n",
    "        if not df.iloc[row].isnull().all() and not df.iloc[row+1].isnull().all() :\n",
    "            #para asegurarse de que no hay filas con cero?\n",
    "            if df.iloc[row].to_numpy().nonzero()[0].shape[0] >0 and  df.iloc[row+1].to_numpy().nonzero()[0].shape[0] >0:\n",
    "                if df.iloc[row][np.argwhere(~np.isnan(df.iloc[row].values)==True)[0][0] ] > 0:\n",
    "                    i = np.argwhere(~np.isnan(df.iloc[row].values)==True)[0][0] #donde estoy (que estado)\n",
    "                    j = np.argwhere(~np.isnan(df.iloc[row+1].values)==True)[0][0] #siguiente linea ( o estado) (hacia que\n",
    "                    # estado voy)\n",
    "                    #revisar si hay mas de un 1, que no deberia de haber\n",
    "                    #        if len( np.nonzero(dfclpw.iloc[row,:].values)) >1:\n",
    "                    #            print('si')\n",
    "\n",
    "                    mattra[i,j] +=1\n",
    "    #convertir de numpy array to dataframe\n",
    "    idxclnames ={}\n",
    "    clidx = range(0,n_clusters)\n",
    "    columnas=[]\n",
    "    for i in np.arange(1,n_clusters+1):\n",
    "        columnas.append('C'+str(i))\n",
    "    clnames = []\n",
    "    for i in range(n_clusters):\n",
    "        clnames.append('C'+str(i+1))\n",
    "    for i in clidx:\n",
    "        idxclnames[i] = columnas[i]\n",
    "\n",
    "    df_mat_tra =pd.DataFrame(mattra)\n",
    "    df_mat_tra.columns=df.columns\n",
    "    #df_mat_tra.rename(index=df.columns)\n",
    "    df_mat_tra.set_index(df.columns,inplace=True)\n",
    "    #la funcion se aplica fila por fila\n",
    "    #se suman todos los valores por fila y se divide entre cada valor, luego se multiplica por 100\n",
    "    #mprob = df_mat_tra.apply(calculate_vectorprob, axis=1)\n",
    "\n",
    "    mprob = df_mat_tra.apply(lambda row: row.astype(float)/row.sum()*100, axis=1)\n",
    "    mprob.rename(index=idxclnames,inplace=True)\n",
    "    if mat_mode == 'prob' : return mprob\n",
    "    else: return df_mat_tra\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# crear dataframe de datos agrupados en clusters (dfclvv, no se usa la función kmeans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# def create_clustered_data(df_comp_vel,df_to_clus,klabels):\n",
    "#     \"\"\"\n",
    "#     Crea un dataframe donde los datos de velocidades estan agrupados en clusters, en formato:\n",
    "#     ---------------------------\n",
    "#     |timestamp | C1 | C2 | C3 |\n",
    "#     |----------|----|----|----|\n",
    "#     |date      | vv |NaN |NaN |\n",
    "#     |----------|----|----|----|\n",
    "#     |date2     |NaN |vv2 |NaN |\n",
    "#     |----------|----|----|----|\n",
    "#     |date3     |NaN |NaN |vv3 |\n",
    "#     | ________________________|\n",
    "#\n",
    "#\n",
    "#     :param df_comp_vel: df con con los componentes de velocidad vx y vy, se utiliza para ordenar los clusters por\n",
    "#     magnitud de viento, tal vez no se necesite\n",
    "#     :param df_to_clus: df con datos de viento que se va a agrupar en clusters.\n",
    "#     :param klabels: las labels resultantes de aplicar kmeans al conjunto de datos.\n",
    "#     :return: df con datos de viento agrupados en clusters, dfclvv\n",
    "#     \"\"\"\n",
    "#     #orden de aparicion de los clusters kmeans\n",
    "#     #ordenar el orden de aparicion segun la magnitud de la vv\n",
    "#     n_clusters = len(np.unique(klabels))\n",
    "#     clmagni = np.zeros(n_clusters)\n",
    "#     for i in range(n_clusters):\n",
    "#         vx = df_comp_vel.vx.values[klabels==i]\n",
    "#         vy = df_comp_vel.vy.values[klabels==i]\n",
    "#         clmagni[i]= np.round(np.mean(np.sqrt(vx**2 + vy**2)),1) #magnitud de la vv\n",
    "#\n",
    "#     clord = clmagni.argsort()\n",
    "#\n",
    "#     columnas=[]\n",
    "#     for i in np.arange(1,n_clusters+1):\n",
    "#         columnas.append('C'+str(i))\n",
    "#\n",
    "#     dfclvv = pd.DataFrame()\n",
    "#     for i in range(n_clusters):\n",
    "#         dfclvv = pd.concat([dfclvv,df_to_clus.vwind[klabels==clord[i]]], ignore_index=True, axis=1)\n",
    "#     dfclvv.columns=columnas[0:n_clusters]\n",
    "#     return dfclvv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_clustered_data(df_to_clus,klabels):\n",
    "    \"\"\"\n",
    "    Crea un dataframe donde los datos de velocidades estan agrupados en clusters, en formato:\n",
    "    ---------------------------\n",
    "    |timestamp | C1 | C2 | C3 |\n",
    "    |----------|----|----|----|\n",
    "    |date      | vv |NaN |NaN |\n",
    "    |----------|----|----|----|\n",
    "    |date2     |NaN |vv2 |NaN |\n",
    "    |----------|----|----|----|\n",
    "    |date3     |NaN |NaN |vv3 |\n",
    "    | ________________________|\n",
    "\n",
    "\n",
    "    :param df_comp_vel: df con con los componentes de velocidad vx y vy, se utiliza para ordenar los clusters por\n",
    "    magnitud de viento, tal vez no se necesite\n",
    "    :param df_to_clus: df con datos de viento que se va a agrupar en clusters.\n",
    "    :param klabels: las labels resultantes de aplicar kmeans al conjunto de datos.\n",
    "    :return: df con datos de viento agrupados en clusters, dfclvv\n",
    "    \"\"\"\n",
    "    #orden de aparicion de los clusters kmeans\n",
    "    #ordenar el orden de aparicion segun la magnitud de la vv\n",
    "    n_clusters = len(np.unique(klabels))\n",
    "    clmagni = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        clmagni[i]= df_to_clus.vwind[klabels==i].mean()\n",
    "\n",
    "\n",
    "    clord = clmagni.argsort()\n",
    "\n",
    "    columnas=[]\n",
    "    for i in np.arange(1,n_clusters+1):\n",
    "        columnas.append('C'+str(i))\n",
    "\n",
    "    dfclvv = pd.DataFrame()\n",
    "    for i in range(n_clusters):\n",
    "        dfclvv = pd.concat([dfclvv,df_to_clus.vwind[klabels==clord[i]]], ignore_index=True, axis=1)\n",
    "    dfclvv.columns=columnas[0:n_clusters]\n",
    "    return dfclvv"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Matriz  a vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mat_to_vector(df):\n",
    "    \"\"\"\n",
    "    #convertir de matriz a vector  (debe ser igual que kmeans_labels cuando la matriz es la original)\n",
    "    #tomo el dataframe con los estados ya indicados y la convierto a vector ignorando\n",
    "    #en este caso NaN's\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vect_states= []\n",
    "    for row in range(len(df)):\n",
    "        clust = np.argwhere(df.iloc[row].notnull().values)[0][0]\n",
    "        vect_states.append(clust)\n",
    "        #else:\n",
    "        #es una fila de ceros, entonces se agrega un cero, así si queda igual\n",
    "        #que kmeans labels y los otros codigos para hacer la matriz de transicion\n",
    "        #pero quiero esto?, no, no lo quiero por que no quiero que de un estado x\n",
    "        #se regrese al estado cero solo porque no hay datos, pero de todos modos lo hago\n",
    "        #vect_states.append(0)\n",
    "    return vect_states\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (aero)",
   "language": "python",
   "name": "pycharm-a777efb8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
